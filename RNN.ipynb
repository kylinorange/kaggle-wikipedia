{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time\n",
    "from subprocess import check_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_1.csv\n",
      "sample_submission_1.csv\n",
      "spider.txt\n",
      "train_1.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_paths = [\n",
    "    \"/data/kaggle-wikipedia/\",\n",
    "    \"/Users/jiayou/Dropbox/JuanCode/Kaggle/Wikipedia/data/\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/JuanCode/Kaggle/Wikipedia/data/\"\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "print(check_output([\"ls\", root]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(root + 'train_1.csv')\n",
    "train.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os, imp\n",
    "import pprint as pp\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]\n",
    "\n",
    "class TFBaseModel(object):\n",
    "\n",
    "    \"\"\"Interface containing some boilerplate code for training tensorflow models.\n",
    "    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n",
    "    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n",
    "    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n",
    "    and ending with the loss tensor.\n",
    "    Args:\n",
    "        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n",
    "            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n",
    "        batch_size: Minibatch size.\n",
    "        learning_rate: Learning rate.\n",
    "        optimizer: 'rms' for RMSProp, 'adam' for Adam, 'sgd' for SGD\n",
    "        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "        regularization_constant:  Regularization constant applied to all trainable parameters.\n",
    "        early_stopping_steps:  Number of steps to continue training after validation loss has\n",
    "            stopped decreasing.\n",
    "        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n",
    "            at warm_start_init_step.\n",
    "        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n",
    "            learning rate will be halved.  This process will repeat num_restarts times.\n",
    "        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n",
    "            to separate checkpoint file.\n",
    "        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n",
    "            have passed.\n",
    "        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n",
    "        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n",
    "            training steps.\n",
    "        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n",
    "        log_dir: Directory where logs are written.\n",
    "        checkpoint_dir: Directory where checkpoints are saved.\n",
    "        prediction_dir: Directory where predictions/outputs are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reader,\n",
    "        batch_size=128,\n",
    "        num_training_steps=20000,\n",
    "        learning_rate=.01,\n",
    "        optimizer='adam',\n",
    "        grad_clip=5,\n",
    "        regularization_constant=0.0,\n",
    "        early_stopping_steps=3000,\n",
    "        warm_start_init_step=0,\n",
    "        num_restarts=None,\n",
    "        enable_parameter_averaging=False,\n",
    "        min_steps_to_checkpoint=100,\n",
    "        log_interval=20,\n",
    "        loss_averaging_window=100,\n",
    "        num_validation_batches=1,\n",
    "        work_dir='tf-data',\n",
    "        name='nn'\n",
    "    ):\n",
    "\n",
    "        self.reader = reader\n",
    "        self.batch_size = batch_size\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_clip = grad_clip\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.warm_start_init_step = warm_start_init_step\n",
    "        self.early_stopping_steps = early_stopping_steps if early_stopping_steps is not None else np.inf\n",
    "        self.enable_parameter_averaging = enable_parameter_averaging\n",
    "        self.num_restarts = num_restarts\n",
    "        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n",
    "        self.log_interval = log_interval\n",
    "        self.num_validation_batches = num_validation_batches\n",
    "        self.loss_averaging_window = loss_averaging_window\n",
    "        self.name = name\n",
    "\n",
    "        self.log_dir = os.path.join(work_dir, 'logs')\n",
    "        self.prediction_dir = os.path.join(work_dir, 'predictions')\n",
    "        self.checkpoint_dir = os.path.join(work_dir, 'checkpoints')\n",
    "        if self.enable_parameter_averaging:\n",
    "            self.checkpoint_dir_averaged = os.path.join(work_dir, 'checkpoints-avg')\n",
    "\n",
    "        self.init_logging(self.log_dir)\n",
    "        self.logger.info('\\nNetwork hyper-parameters:\\n{}'.format(pp.pformat(self.__dict__)))\n",
    "        self.reader.describe(self.logger)\n",
    "\n",
    "        self.graph = self.build_graph()\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        print('Built graph')\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        raise NotImplementedError('subclass must implement this')\n",
    "\n",
    "    def fit(self):\n",
    "        with self.session.as_default():\n",
    "\n",
    "            if self.warm_start_init_step:\n",
    "                self.restore(self.warm_start_init_step)\n",
    "                step = self.warm_start_init_step\n",
    "            else:\n",
    "                self.session.run(self.init)\n",
    "                step = 0\n",
    "\n",
    "            train_generator = self.reader.train_batch_generator(self.batch_size)\n",
    "            val_generator = self.reader.val_batch_generator(self.num_validation_batches*self.batch_size)\n",
    "\n",
    "            train_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "            val_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "\n",
    "            best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "            restarts = 0\n",
    "\n",
    "            while step < self.num_training_steps:\n",
    "\n",
    "                # validation evaluation\n",
    "                val_batch_df = next(val_generator)\n",
    "                val_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in val_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                val_feed_dict.update({self.learning_rate_var: self.learning_rate})\n",
    "                [val_loss] = self.session.run(\n",
    "                    fetches=[self.loss],\n",
    "                    feed_dict=val_feed_dict\n",
    "                )\n",
    "                val_loss_history.append(val_loss)\n",
    "\n",
    "                # train step\n",
    "                train_batch_df = next(train_generator)\n",
    "                train_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in train_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                train_feed_dict.update({self.learning_rate_var: self.learning_rate})\n",
    "                train_loss, _ = self.session.run(\n",
    "                    fetches=[self.loss, self.step],\n",
    "                    feed_dict=train_feed_dict\n",
    "                )\n",
    "                train_loss_history.append(train_loss)\n",
    "\n",
    "                if step % self.log_interval == 0 or step + 1 == self.num_training_steps:\n",
    "                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
    "                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
    "                    metric_log = (\n",
    "                        \"[[step {:>8}]]     \"\n",
    "                        \"[[train]]     loss: {:<12}     \"\n",
    "                        \"[[val]]     loss: {:<12}\"\n",
    "                    ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n",
    "                    self.logger.info(metric_log)\n",
    "\n",
    "                    if avg_val_loss < best_validation_loss:\n",
    "                        best_validation_loss = avg_val_loss\n",
    "                        best_validation_tstep = step\n",
    "                        if step > self.min_steps_to_checkpoint:\n",
    "                            self.save(step)\n",
    "                            if self.enable_parameter_averaging:\n",
    "                                self.save(step, averaged=True)\n",
    "\n",
    "                    if step - best_validation_tstep >= self.early_stopping_steps:\n",
    "\n",
    "                        if self.num_restarts is None or restarts >= self.num_restarts:\n",
    "                            self.logger.info('Early stopping')\n",
    "                            break\n",
    "\n",
    "                        if restarts < self.num_restarts:\n",
    "                            self.logger.info('')\n",
    "                            self.restore(best_validation_tstep)\n",
    "                            self.learning_rate /= 2.0\n",
    "#                             self.early_stopping_steps /= 2\n",
    "                            step = best_validation_tstep\n",
    "                            restarts += 1\n",
    "                            self.logger.info(\n",
    "                                'Half learning rate to {} and restore step {}'.format(self.learning_rate, step)\n",
    "                            )\n",
    "\n",
    "                step += 1\n",
    "\n",
    "            if step <= self.min_steps_to_checkpoint:\n",
    "                best_validation_tstep = step\n",
    "                self.save(step)\n",
    "                if self.enable_parameter_averaging:\n",
    "                    self.save(step, averaged=True)\n",
    "\n",
    "            self.logger.info('Training ended')\n",
    "            self.logger.info(\n",
    "                'Best validation loss of {} at training step {}'.format(\n",
    "                    round(best_validation_loss, 8), \n",
    "                    best_validation_tstep\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def predict(self, chunk_size=2048):\n",
    "        if not os.path.isdir(self.prediction_dir):\n",
    "            os.makedirs(self.prediction_dir)\n",
    "\n",
    "        preds = {}\n",
    "        if hasattr(self, 'prediction_tensors'):\n",
    "            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n",
    "\n",
    "            test_generator = self.reader.test_batch_generator(chunk_size)\n",
    "            for i, test_batch_df in enumerate(test_generator):\n",
    "                test_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n",
    "                np_tensors = self.session.run(\n",
    "                    fetches=tf_tensors,\n",
    "                    feed_dict=test_feed_dict\n",
    "                )\n",
    "                for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "                    prediction_dict[tensor_name].append(tensor)\n",
    "\n",
    "            for tensor_name, tensor in prediction_dict.items():\n",
    "                np_tensor = np.concatenate(tensor, 0)\n",
    "                preds[tensor_name] = np_tensor\n",
    "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "                self.logger.info('Saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "                np.save(save_file, np_tensor)\n",
    "\n",
    "        if hasattr(self, 'parameter_tensors'):\n",
    "            for tensor_name, tensor in self.parameter_tensors.items():\n",
    "                np_tensor = tensor.eval(self.session)\n",
    "\n",
    "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "                self.logger.info('Saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "                np.save(save_file, np_tensor)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def save(self, step, averaged=False):\n",
    "        saver = self.saver_averaged if averaged else self.saver\n",
    "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
    "        if not os.path.isdir(checkpoint_dir):\n",
    "            self.logger.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
    "            os.mkdir(checkpoint_dir)\n",
    "\n",
    "        model_path = os.path.join(checkpoint_dir, 'model')\n",
    "        self.logger.info('saving model to {}'.format(model_path))\n",
    "        saver.save(self.session, model_path, global_step=step)\n",
    "\n",
    "    def restore(self, step=None, averaged=False):\n",
    "        saver = self.saver_averaged if averaged else self.saver\n",
    "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
    "        if not step:\n",
    "            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "            self.logger.info('Restoring model parameters from {}'.format(model_path))\n",
    "            saver.restore(self.session, model_path)\n",
    "        else:\n",
    "            model_path = os.path.join(\n",
    "                checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
    "            )\n",
    "            self.logger.info('Restoring model from {}'.format(model_path))\n",
    "            saver.restore(self.session, model_path)\n",
    "\n",
    "    def init_logging(self, log_dir):\n",
    "        if not os.path.isdir(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        logger = logging.getLogger(\n",
    "            '{}.{}'.format(\n",
    "                type(self).__name__, \n",
    "                datetime.now().strftime('%Y-%m-%d.%H-%M-%S.%f')\n",
    "            )\n",
    "        )\n",
    "        logger.setLevel(logging.INFO)\n",
    "        fmtr = logging.Formatter(\n",
    "            fmt='[[%(asctime)s]] %(message)s',\n",
    "            datefmt='%m/%d/%Y %I:%M:%S %p'\n",
    "        )\n",
    "\n",
    "        h = logging.StreamHandler(stream=sys.stdout)\n",
    "        h.setFormatter(fmtr)\n",
    "        logger.addHandler(h)\n",
    "        \n",
    "        date_str = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        log_file = 'log.{}.{}.{}.txt'.format(type(self).__name__, self.name, date_str)\n",
    "        h = logging.FileHandler(filename=os.path.join(log_dir, log_file))\n",
    "        h.setFormatter(fmtr)\n",
    "        logger.addHandler(h)\n",
    "        \n",
    "        self.logger = logger\n",
    "\n",
    "    def update_parameters(self, loss):\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.learning_rate_var = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        if self.regularization_constant != 0:\n",
    "            l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n",
    "            loss = loss + self.regularization_constant*l2_norm\n",
    "\n",
    "        optimizer = self.get_optimizer(self.learning_rate_var)\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        clipped = [(tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads]\n",
    "\n",
    "        step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n",
    "\n",
    "        if self.enable_parameter_averaging:\n",
    "            maintain_averages_op = self.ema.apply(tf.trainable_variables())\n",
    "            with tf.control_dependencies([step]):\n",
    "                self.step = tf.group(maintain_averages_op)\n",
    "        else:\n",
    "            self.step = step\n",
    "\n",
    "    def log_parameters(self):\n",
    "        self.logger.info(\n",
    "            (\n",
    "                '\\n\\n'\n",
    "                'All parameters:\\n{}\\n'\n",
    "                'Trainable parameters:\\n{}\\n'\n",
    "                'Trainable parameter count: {}\\n'\n",
    "            ).format(\n",
    "                pp.pformat([(var.name, shape(var)) for var in tf.global_variables()]),\n",
    "                pp.pformat([(var.name, shape(var)) for var in tf.trainable_variables()]),\n",
    "                str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_optimizer(self, learning_rate):\n",
    "        if self.optimizer == 'adam':\n",
    "            return tf.train.AdamOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'gd':\n",
    "            return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'rms':\n",
    "            return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "        else:\n",
    "            assert False, 'optimizer must be adam, gd, or rms'\n",
    "\n",
    "    def build_graph(self):\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            self.ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "\n",
    "            self.loss = self.calculate_loss()\n",
    "            self.update_parameters(self.loss)\n",
    "            self.log_parameters()\n",
    "\n",
    "            self.saver = tf.train.Saver(max_to_keep=1)\n",
    "            if self.enable_parameter_averaging:\n",
    "                self.saver_averaged = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "            return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self, data, seed=923):\n",
    "        self.data = data.iloc[:,1:].values\n",
    "        self.seed = seed\n",
    "        self.days = self.data.shape[1]\n",
    "\n",
    "        \n",
    "    def describe(self, logger):\n",
    "        logger.info('')\n",
    "        logger.info('Data dimensions:')\n",
    "        logger.info('    [[data]] {}'.format(self.data.shape))\n",
    "        logger.info('Split seed = {}'.format(self.seed))\n",
    "        logger.info('')\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            self.data,\n",
    "            batch_size=batch_size,\n",
    "            given_days=self.days - 62,\n",
    "            no_loss_days=30,\n",
    "            days=self.days - 62,\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            self.data,\n",
    "            batch_size=batch_size,\n",
    "            given_days=self.days - 62,\n",
    "            no_loss_days=self.days - 62,\n",
    "            days=self.days,\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        pass\n",
    "\n",
    "    def batch_generator(self, data, batch_size, given_days, no_loss_days, days):\n",
    "        while True:\n",
    "            batch = {}\n",
    "            idx = np.random.randint(0, data.shape[0], [batch_size])\n",
    "            batch['data'] = data[idx, :]\n",
    "            batch['given_days'] = given_days\n",
    "            batch['no_loss_days'] = no_loss_days\n",
    "            batch['days'] = days\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiRNN(TFBaseModel):\n",
    "\n",
    "    def __init__(self, state_size, keep_prob=1, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        super(type(self), self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        self.data = tf.placeholder(tf.float32, name='data')\n",
    "        self.given_days = tf.placeholder(tf.int32, name='given_days')\n",
    "        self.no_loss_days = tf.placeholder(tf.int32, name='no_loss_days')\n",
    "        self.days = tf.placeholder(tf.int32, name='days')\n",
    "        \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(\n",
    "                self.state_size\n",
    "            ),\n",
    "            output_keep_prob=self.keep_prob\n",
    "        )\n",
    "        \n",
    "        # [batch_size, state_size]\n",
    "        state = cell.zero_state(tf.shape(self.data)[0], dtype=tf.float32)\n",
    "        # [batch_size, 1]\n",
    "        last_output = tf.zeros([tf.shape(self.data)[0], 1], dtype=tf.float32)\n",
    "        \n",
    "        loss = tf.constant(0, dtype=tf.float32)\n",
    "        step = tf.constant(0, dtype=tf.int32)\n",
    "        \n",
    "        def cond(last_output, state, loss, step):\n",
    "            return step < self.days\n",
    "        \n",
    "        def body(last_output, state, loss, step):\n",
    "            output, state = cell(last_output, state)\n",
    "            output = tf.layers.dense(\n",
    "                output,\n",
    "                1,\n",
    "                activation=tf.nn.relu,\n",
    "                name='dense-top'\n",
    "            )\n",
    "            \n",
    "            last_output = tf.cond(\n",
    "                step < self.given_days,\n",
    "                lambda: tf.expand_dims(self.data[:,step], 1),\n",
    "                lambda: output,\n",
    "            )\n",
    "            last_output.set_shape([None, 1])\n",
    "            last_output = tf.round(last_output)\n",
    "            \n",
    "            loss = tf.cond(\n",
    "                step >= self.no_loss_days,\n",
    "                lambda: \\\n",
    "                    loss + \\\n",
    "                    tf.reduce_mean(2 * tf.abs(self.data[:,step] - output) \\\n",
    "                    / tf.maximum(1e-8, self.data[:,step] + output)),\n",
    "                lambda: loss\n",
    "            )\n",
    "            loss.set_shape([])\n",
    "            \n",
    "            return (last_output, state, loss, step + 1)\n",
    "        \n",
    "        _, _, loss, _ = tf.while_loop(\n",
    "            cond=cond,\n",
    "            body=body,\n",
    "            loop_vars=(last_output, state, loss, step)\n",
    "        )\n",
    "        \n",
    "        return loss / tf.cast(self.days - self.no_loss_days, tf.float32) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = DataReader(train, seed=923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[08/28/2017 03:24:09 PM]] \n",
      "Network hyper-parameters:\n",
      "{'batch_size': 1,\n",
      " 'checkpoint_dir': './tf-data/checkpoints',\n",
      " 'early_stopping_steps': 300,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'grad_clip': 5,\n",
      " 'keep_prob': 1,\n",
      " 'learning_rate': 1.0001,\n",
      " 'log_dir': './tf-data/logs',\n",
      " 'log_interval': 1,\n",
      " 'logger': <Logger WikiRNN.2017-08-28.15-24-09.406871 (INFO)>,\n",
      " 'loss_averaging_window': 1,\n",
      " 'min_steps_to_checkpoint': 0,\n",
      " 'name': 'v0',\n",
      " 'num_restarts': 3,\n",
      " 'num_training_steps': 10000,\n",
      " 'num_validation_batches': 1,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': './tf-data/predictions',\n",
      " 'reader': <__main__.DataReader object at 0x11f70e438>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'state_size': 10,\n",
      " 'warm_start_init_step': 0}\n",
      "[[08/28/2017 03:24:09 PM]] \n",
      "[[08/28/2017 03:24:09 PM]] Data dimensions:\n",
      "[[08/28/2017 03:24:09 PM]]     [[data]] (145063, 550)\n",
      "[[08/28/2017 03:24:09 PM]] Split seed = 923\n",
      "[[08/28/2017 03:24:09 PM]] \n",
      "[[08/28/2017 03:24:10 PM]] \n",
      "\n",
      "All parameters:\n",
      "[('lstm_cell/weights:0', [11, 40]),\n",
      " ('lstm_cell/biases:0', [40]),\n",
      " ('dense-top/kernel:0', [10, 1]),\n",
      " ('dense-top/bias:0', [1]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('lstm_cell/weights/Adam:0', [11, 40]),\n",
      " ('lstm_cell/weights/Adam_1:0', [11, 40]),\n",
      " ('lstm_cell/biases/Adam:0', [40]),\n",
      " ('lstm_cell/biases/Adam_1:0', [40]),\n",
      " ('dense-top/kernel/Adam:0', [10, 1]),\n",
      " ('dense-top/kernel/Adam_1:0', [10, 1]),\n",
      " ('dense-top/bias/Adam:0', [1]),\n",
      " ('dense-top/bias/Adam_1:0', [1])]\n",
      "Trainable parameters:\n",
      "[('lstm_cell/weights:0', [11, 40]),\n",
      " ('lstm_cell/biases:0', [40]),\n",
      " ('dense-top/kernel:0', [10, 1]),\n",
      " ('dense-top/bias:0', [1])]\n",
      "Trainable parameter count: 491\n",
      "\n",
      "Built graph\n",
      "[[08/28/2017 03:24:13 PM]] [[step        0]]     [[train]]     loss: 199.63101196     [[val]]     loss: 195.19396973\n",
      "[[08/28/2017 03:24:15 PM]] [[step        1]]     [[train]]     loss: 152.56826782     [[val]]     loss: 198.9669342 \n",
      "[[08/28/2017 03:24:16 PM]] [[step        2]]     [[train]]     loss: 189.58444214     [[val]]     loss: 199.55407715\n",
      "[[08/28/2017 03:24:18 PM]] [[step        3]]     [[train]]     loss: 78.48215485      [[val]]     loss: 173.13388062\n",
      "[[08/28/2017 03:24:18 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/28/2017 03:24:21 PM]] [[step        4]]     [[train]]     loss: 158.28881836     [[val]]     loss: 52.03875732 \n",
      "[[08/28/2017 03:24:21 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/28/2017 03:24:24 PM]] [[step        5]]     [[train]]     loss: 188.67703247     [[val]]     loss: 193.5244751 \n",
      "[[08/28/2017 03:24:26 PM]] [[step        6]]     [[train]]     loss: 144.0362854      [[val]]     loss: 188.97720337\n",
      "[[08/28/2017 03:24:28 PM]] [[step        7]]     [[train]]     loss: 190.38528442     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:24:29 PM]] [[step        8]]     [[train]]     loss: 41.16819382      [[val]]     loss: 111.98374939\n",
      "[[08/28/2017 03:24:31 PM]] [[step        9]]     [[train]]     loss: 194.07130432     [[val]]     loss: 187.79878235\n",
      "[[08/28/2017 03:24:33 PM]] [[step       10]]     [[train]]     loss: 169.53013611     [[val]]     loss: 142.77780151\n",
      "[[08/28/2017 03:24:35 PM]] [[step       11]]     [[train]]     loss: 178.96246338     [[val]]     loss: 57.84243393 \n",
      "[[08/28/2017 03:24:37 PM]] [[step       12]]     [[train]]     loss: 110.21398163     [[val]]     loss: 140.27424622\n",
      "[[08/28/2017 03:24:39 PM]] [[step       13]]     [[train]]     loss: 154.43730164     [[val]]     loss: 174.12870789\n",
      "[[08/28/2017 03:24:40 PM]] [[step       14]]     [[train]]     loss: 58.86525345      [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:24:42 PM]] [[step       15]]     [[train]]     loss: 186.78330994     [[val]]     loss: 127.49068451\n",
      "[[08/28/2017 03:24:44 PM]] [[step       16]]     [[train]]     loss: 154.58450317     [[val]]     loss: 144.29879761\n",
      "[[08/28/2017 03:24:46 PM]] [[step       17]]     [[train]]     loss: 197.83563232     [[val]]     loss: 102.71131897\n",
      "[[08/28/2017 03:24:48 PM]] [[step       18]]     [[train]]     loss: 199.09101868     [[val]]     loss: 140.24014282\n",
      "[[08/28/2017 03:24:49 PM]] [[step       19]]     [[train]]     loss: 147.1725769      [[val]]     loss: 130.9316864 \n",
      "[[08/28/2017 03:24:51 PM]] [[step       20]]     [[train]]     loss: 188.14863586     [[val]]     loss: 191.67649841\n",
      "[[08/28/2017 03:24:53 PM]] [[step       21]]     [[train]]     loss: 198.48025513     [[val]]     loss: 185.33589172\n",
      "[[08/28/2017 03:24:55 PM]] [[step       22]]     [[train]]     loss: 108.3797226      [[val]]     loss: 62.21902466 \n",
      "[[08/28/2017 03:24:57 PM]] [[step       23]]     [[train]]     loss: 145.12973022     [[val]]     loss: 32.27538681 \n",
      "[[08/28/2017 03:24:57 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/28/2017 03:25:00 PM]] [[step       24]]     [[train]]     loss: 163.31732178     [[val]]     loss: 66.22693634 \n",
      "[[08/28/2017 03:25:01 PM]] [[step       25]]     [[train]]     loss: 162.32341003     [[val]]     loss: 182.534729  \n",
      "[[08/28/2017 03:25:03 PM]] [[step       26]]     [[train]]     loss: 63.77350235      [[val]]     loss: 190.17431641\n",
      "[[08/28/2017 03:25:05 PM]] [[step       27]]     [[train]]     loss: 53.40985107      [[val]]     loss: 31.25002098 \n",
      "[[08/28/2017 03:25:05 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/28/2017 03:25:08 PM]] [[step       28]]     [[train]]     loss: 62.37824631      [[val]]     loss: 197.52897644\n",
      "[[08/28/2017 03:25:10 PM]] [[step       29]]     [[train]]     loss: 38.17559052      [[val]]     loss: 87.025177   \n",
      "[[08/28/2017 03:25:12 PM]] [[step       30]]     [[train]]     loss: 193.92718506     [[val]]     loss: 54.57283783 \n",
      "[[08/28/2017 03:25:14 PM]] [[step       31]]     [[train]]     loss: 183.4744873      [[val]]     loss: 184.10848999\n",
      "[[08/28/2017 03:25:15 PM]] [[step       32]]     [[train]]     loss: 190.56318665     [[val]]     loss: 131.7668457 \n",
      "[[08/28/2017 03:25:17 PM]] [[step       33]]     [[train]]     loss: 89.26402283      [[val]]     loss: 67.73200989 \n",
      "[[08/28/2017 03:25:19 PM]] [[step       34]]     [[train]]     loss: 197.22949219     [[val]]     loss: 195.96810913\n",
      "[[08/28/2017 03:25:21 PM]] [[step       35]]     [[train]]     loss: 171.75427246     [[val]]     loss: 186.3709259 \n",
      "[[08/28/2017 03:25:23 PM]] [[step       36]]     [[train]]     loss: 170.22999573     [[val]]     loss: 144.5406189 \n",
      "[[08/28/2017 03:25:24 PM]] [[step       37]]     [[train]]     loss: 47.72266006      [[val]]     loss: 175.42919922\n",
      "[[08/28/2017 03:25:26 PM]] [[step       38]]     [[train]]     loss: 194.95611572     [[val]]     loss: 53.64935303 \n",
      "[[08/28/2017 03:25:28 PM]] [[step       39]]     [[train]]     loss: 190.46009827     [[val]]     loss: 165.48138428\n",
      "[[08/28/2017 03:25:30 PM]] [[step       40]]     [[train]]     loss: 198.67802429     [[val]]     loss: 134.71731567\n",
      "[[08/28/2017 03:25:32 PM]] [[step       41]]     [[train]]     loss: 189.26655579     [[val]]     loss: 142.48832703\n",
      "[[08/28/2017 03:25:33 PM]] [[step       42]]     [[train]]     loss: 165.40516663     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:25:35 PM]] [[step       43]]     [[train]]     loss: 179.73918152     [[val]]     loss: 49.18013382 \n",
      "[[08/28/2017 03:25:37 PM]] [[step       44]]     [[train]]     loss: 107.55950165     [[val]]     loss: 84.81050873 \n",
      "[[08/28/2017 03:25:39 PM]] [[step       45]]     [[train]]     loss: 96.86940002      [[val]]     loss: 129.272995  \n",
      "[[08/28/2017 03:25:41 PM]] [[step       46]]     [[train]]     loss: 156.9874115      [[val]]     loss: 170.14836121\n",
      "[[08/28/2017 03:25:42 PM]] [[step       47]]     [[train]]     loss: 196.60438538     [[val]]     loss: 196.12658691\n",
      "[[08/28/2017 03:25:44 PM]] [[step       48]]     [[train]]     loss: 80.73630524      [[val]]     loss: 154.89433289\n",
      "[[08/28/2017 03:25:46 PM]] [[step       49]]     [[train]]     loss: 151.83135986     [[val]]     loss: 80.22211456 \n",
      "[[08/28/2017 03:25:48 PM]] [[step       50]]     [[train]]     loss: 193.3452301      [[val]]     loss: 188.43029785\n",
      "[[08/28/2017 03:25:49 PM]] [[step       51]]     [[train]]     loss: 79.54663086      [[val]]     loss: 180.19549561\n",
      "[[08/28/2017 03:25:51 PM]] [[step       52]]     [[train]]     loss: 190.4085083      [[val]]     loss: 74.57486725 \n",
      "[[08/28/2017 03:25:53 PM]] [[step       53]]     [[train]]     loss: 192.00465393     [[val]]     loss: 103.80799866\n",
      "[[08/28/2017 03:25:55 PM]] [[step       54]]     [[train]]     loss: 68.41670227      [[val]]     loss: 152.1933136 \n",
      "[[08/28/2017 03:25:56 PM]] [[step       55]]     [[train]]     loss: 200.0            [[val]]     loss: 158.66168213\n",
      "[[08/28/2017 03:25:58 PM]] [[step       56]]     [[train]]     loss: 110.87410736     [[val]]     loss: 198.79315186\n",
      "[[08/28/2017 03:26:00 PM]] [[step       57]]     [[train]]     loss: 120.41356659     [[val]]     loss: 191.13116455\n",
      "[[08/28/2017 03:26:02 PM]] [[step       58]]     [[train]]     loss: 173.48919678     [[val]]     loss: 185.41030884\n",
      "[[08/28/2017 03:26:03 PM]] [[step       59]]     [[train]]     loss: 180.43746948     [[val]]     loss: 126.42894745\n",
      "[[08/28/2017 03:26:05 PM]] [[step       60]]     [[train]]     loss: 187.06192017     [[val]]     loss: 116.68686676\n",
      "[[08/28/2017 03:26:07 PM]] [[step       61]]     [[train]]     loss: 52.42296982      [[val]]     loss: 188.00488281\n",
      "[[08/28/2017 03:26:09 PM]] [[step       62]]     [[train]]     loss: 41.93740463      [[val]]     loss: 55.07508087 \n",
      "[[08/28/2017 03:26:10 PM]] [[step       63]]     [[train]]     loss: 75.58808899      [[val]]     loss: 196.58612061\n",
      "[[08/28/2017 03:26:12 PM]] [[step       64]]     [[train]]     loss: 182.40931702     [[val]]     loss: 199.82177734\n",
      "[[08/28/2017 03:26:14 PM]] [[step       65]]     [[train]]     loss: 194.13143921     [[val]]     loss: 83.28585052 \n",
      "[[08/28/2017 03:26:16 PM]] [[step       66]]     [[train]]     loss: 198.0957489      [[val]]     loss: 179.12319946\n",
      "[[08/28/2017 03:26:17 PM]] [[step       67]]     [[train]]     loss: 61.4513092       [[val]]     loss: 165.27304077\n",
      "[[08/28/2017 03:26:19 PM]] [[step       68]]     [[train]]     loss: 89.15955353      [[val]]     loss: 174.89349365\n",
      "[[08/28/2017 03:26:21 PM]] [[step       69]]     [[train]]     loss: 46.92815399      [[val]]     loss: 176.29130554\n",
      "[[08/28/2017 03:26:23 PM]] [[step       70]]     [[train]]     loss: 69.86112976      [[val]]     loss: 107.06261444\n",
      "[[08/28/2017 03:26:24 PM]] [[step       71]]     [[train]]     loss: 89.174263        [[val]]     loss: 171.99723816\n",
      "[[08/28/2017 03:26:26 PM]] [[step       72]]     [[train]]     loss: 89.34432983      [[val]]     loss: 161.01673889\n",
      "[[08/28/2017 03:26:28 PM]] [[step       73]]     [[train]]     loss: 175.49812317     [[val]]     loss: 183.44474792\n",
      "[[08/28/2017 03:26:30 PM]] [[step       74]]     [[train]]     loss: 175.28804016     [[val]]     loss: 198.96917725\n",
      "[[08/28/2017 03:26:31 PM]] [[step       75]]     [[train]]     loss: 195.98519897     [[val]]     loss: 68.5870285  \n",
      "[[08/28/2017 03:26:33 PM]] [[step       76]]     [[train]]     loss: 195.00892639     [[val]]     loss: 93.96827698 \n",
      "[[08/28/2017 03:26:35 PM]] [[step       77]]     [[train]]     loss: 181.5675354      [[val]]     loss: 186.03915405\n",
      "[[08/28/2017 03:26:37 PM]] [[step       78]]     [[train]]     loss: 197.33404541     [[val]]     loss: 197.22235107\n",
      "[[08/28/2017 03:26:38 PM]] [[step       79]]     [[train]]     loss: 45.90639877      [[val]]     loss: 189.50978088\n",
      "[[08/28/2017 03:26:40 PM]] [[step       80]]     [[train]]     loss: 56.95707321      [[val]]     loss: 197.98632812\n",
      "[[08/28/2017 03:26:42 PM]] [[step       81]]     [[train]]     loss: 40.72489166      [[val]]     loss: 45.0005722  \n",
      "[[08/28/2017 03:26:44 PM]] [[step       82]]     [[train]]     loss: 150.63360596     [[val]]     loss: 193.83978271\n",
      "[[08/28/2017 03:26:45 PM]] [[step       83]]     [[train]]     loss: 60.95087051      [[val]]     loss: 194.29701233\n",
      "[[08/28/2017 03:26:47 PM]] [[step       84]]     [[train]]     loss: 74.526474        [[val]]     loss: 164.5259552 \n",
      "[[08/28/2017 03:26:49 PM]] [[step       85]]     [[train]]     loss: 144.99154663     [[val]]     loss: 195.93531799\n",
      "[[08/28/2017 03:26:51 PM]] [[step       86]]     [[train]]     loss: 200.0            [[val]]     loss: 192.9697113 \n",
      "[[08/28/2017 03:26:52 PM]] [[step       87]]     [[train]]     loss: 29.67211533      [[val]]     loss: 174.94058228\n",
      "[[08/28/2017 03:26:54 PM]] [[step       88]]     [[train]]     loss: 198.93186951     [[val]]     loss: 170.33601379\n",
      "[[08/28/2017 03:26:56 PM]] [[step       89]]     [[train]]     loss: 123.03534698     [[val]]     loss: 101.11942291\n",
      "[[08/28/2017 03:26:58 PM]] [[step       90]]     [[train]]     loss: 106.54042816     [[val]]     loss: 150.01312256\n",
      "[[08/28/2017 03:26:59 PM]] [[step       91]]     [[train]]     loss: 124.87530518     [[val]]     loss: 114.93386078\n",
      "[[08/28/2017 03:27:01 PM]] [[step       92]]     [[train]]     loss: 116.47902679     [[val]]     loss: 170.19378662\n",
      "[[08/28/2017 03:27:03 PM]] [[step       93]]     [[train]]     loss: 162.82099915     [[val]]     loss: 198.57589722\n",
      "[[08/28/2017 03:27:05 PM]] [[step       94]]     [[train]]     loss: 186.80299377     [[val]]     loss: 70.32870483 \n",
      "[[08/28/2017 03:27:07 PM]] [[step       95]]     [[train]]     loss: 70.67865753      [[val]]     loss: 185.36631775\n",
      "[[08/28/2017 03:27:08 PM]] [[step       96]]     [[train]]     loss: 103.9362793      [[val]]     loss: 99.50920105 \n",
      "[[08/28/2017 03:27:10 PM]] [[step       97]]     [[train]]     loss: 44.21185684      [[val]]     loss: 125.64044189\n",
      "[[08/28/2017 03:27:12 PM]] [[step       98]]     [[train]]     loss: 152.00205994     [[val]]     loss: 112.95801544\n",
      "[[08/28/2017 03:27:14 PM]] [[step       99]]     [[train]]     loss: 115.01313782     [[val]]     loss: 110.09481812\n",
      "[[08/28/2017 03:27:15 PM]] [[step      100]]     [[train]]     loss: 32.45355606      [[val]]     loss: 121.24321747\n",
      "[[08/28/2017 03:27:17 PM]] [[step      101]]     [[train]]     loss: 126.27020264     [[val]]     loss: 125.40488434\n",
      "[[08/28/2017 03:27:19 PM]] [[step      102]]     [[train]]     loss: 196.16304016     [[val]]     loss: 189.36262512\n",
      "[[08/28/2017 03:27:20 PM]] [[step      103]]     [[train]]     loss: 167.76545715     [[val]]     loss: 61.52919006 \n",
      "[[08/28/2017 03:27:22 PM]] [[step      104]]     [[train]]     loss: 91.78566742      [[val]]     loss: 153.98551941\n",
      "[[08/28/2017 03:27:24 PM]] [[step      105]]     [[train]]     loss: 142.37940979     [[val]]     loss: 187.78898621\n",
      "[[08/28/2017 03:27:26 PM]] [[step      106]]     [[train]]     loss: 158.26893616     [[val]]     loss: 56.83549118 \n",
      "[[08/28/2017 03:27:28 PM]] [[step      107]]     [[train]]     loss: 187.73092651     [[val]]     loss: 131.05183411\n",
      "[[08/28/2017 03:27:29 PM]] [[step      108]]     [[train]]     loss: 167.85980225     [[val]]     loss: 184.95376587\n",
      "[[08/28/2017 03:27:31 PM]] [[step      109]]     [[train]]     loss: 112.74761963     [[val]]     loss: 54.98310852 \n",
      "[[08/28/2017 03:27:33 PM]] [[step      110]]     [[train]]     loss: 115.83135986     [[val]]     loss: 167.18515015\n",
      "[[08/28/2017 03:27:35 PM]] [[step      111]]     [[train]]     loss: 157.17024231     [[val]]     loss: 101.78231812\n",
      "[[08/28/2017 03:27:36 PM]] [[step      112]]     [[train]]     loss: 41.33494949      [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:27:38 PM]] [[step      113]]     [[train]]     loss: 171.70727539     [[val]]     loss: 57.28458786 \n",
      "[[08/28/2017 03:27:40 PM]] [[step      114]]     [[train]]     loss: 187.15301514     [[val]]     loss: 152.36329651\n",
      "[[08/28/2017 03:27:42 PM]] [[step      115]]     [[train]]     loss: 149.07725525     [[val]]     loss: 154.65744019\n",
      "[[08/28/2017 03:27:43 PM]] [[step      116]]     [[train]]     loss: 171.06195068     [[val]]     loss: 122.3513031 \n",
      "[[08/28/2017 03:27:45 PM]] [[step      117]]     [[train]]     loss: 183.1915741      [[val]]     loss: 156.67150879\n",
      "[[08/28/2017 03:27:47 PM]] [[step      118]]     [[train]]     loss: 75.1583786       [[val]]     loss: 183.1061554 \n",
      "[[08/28/2017 03:27:49 PM]] [[step      119]]     [[train]]     loss: 138.71604919     [[val]]     loss: 128.62423706\n",
      "[[08/28/2017 03:27:50 PM]] [[step      120]]     [[train]]     loss: 96.77577209      [[val]]     loss: 185.41877747\n",
      "[[08/28/2017 03:27:52 PM]] [[step      121]]     [[train]]     loss: 148.58197021     [[val]]     loss: 172.61857605\n",
      "[[08/28/2017 03:27:54 PM]] [[step      122]]     [[train]]     loss: 173.71482849     [[val]]     loss: 72.88710785 \n",
      "[[08/28/2017 03:27:56 PM]] [[step      123]]     [[train]]     loss: 85.59095001      [[val]]     loss: 95.58847809 \n",
      "[[08/28/2017 03:27:57 PM]] [[step      124]]     [[train]]     loss: 137.76177979     [[val]]     loss: 190.35462952\n",
      "[[08/28/2017 03:27:59 PM]] [[step      125]]     [[train]]     loss: 102.4440918      [[val]]     loss: 187.81678772\n",
      "[[08/28/2017 03:28:01 PM]] [[step      126]]     [[train]]     loss: 167.37171936     [[val]]     loss: 70.5925293  \n",
      "[[08/28/2017 03:28:03 PM]] [[step      127]]     [[train]]     loss: 146.05967712     [[val]]     loss: 138.79872131\n",
      "[[08/28/2017 03:28:04 PM]] [[step      128]]     [[train]]     loss: 166.68284607     [[val]]     loss: 60.70398712 \n",
      "[[08/28/2017 03:28:06 PM]] [[step      129]]     [[train]]     loss: 191.85520935     [[val]]     loss: 184.81822205\n",
      "[[08/28/2017 03:28:08 PM]] [[step      130]]     [[train]]     loss: 197.17337036     [[val]]     loss: 57.13279724 \n",
      "[[08/28/2017 03:28:10 PM]] [[step      131]]     [[train]]     loss: 69.70385742      [[val]]     loss: 74.61921692 \n",
      "[[08/28/2017 03:28:11 PM]] [[step      132]]     [[train]]     loss: 91.89481354      [[val]]     loss: 140.4425354 \n",
      "[[08/28/2017 03:28:13 PM]] [[step      133]]     [[train]]     loss: 182.8400116      [[val]]     loss: 164.54901123\n",
      "[[08/28/2017 03:28:15 PM]] [[step      134]]     [[train]]     loss: 141.4884491      [[val]]     loss: 39.55168152 \n",
      "[[08/28/2017 03:28:17 PM]] [[step      135]]     [[train]]     loss: 125.50366974     [[val]]     loss: 171.72129822\n",
      "[[08/28/2017 03:28:18 PM]] [[step      136]]     [[train]]     loss: 196.21826172     [[val]]     loss: 153.85566711\n",
      "[[08/28/2017 03:28:20 PM]] [[step      137]]     [[train]]     loss: 117.76039886     [[val]]     loss: 107.88910675\n",
      "[[08/28/2017 03:28:22 PM]] [[step      138]]     [[train]]     loss: 26.070261        [[val]]     loss: 141.94790649\n",
      "[[08/28/2017 03:28:24 PM]] [[step      139]]     [[train]]     loss: 152.28034973     [[val]]     loss: 58.80652618 \n",
      "[[08/28/2017 03:28:25 PM]] [[step      140]]     [[train]]     loss: 198.01119995     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:28:27 PM]] [[step      141]]     [[train]]     loss: 176.45727539     [[val]]     loss: 141.55456543\n",
      "[[08/28/2017 03:28:29 PM]] [[step      142]]     [[train]]     loss: 101.70614624     [[val]]     loss: 101.29316711\n",
      "[[08/28/2017 03:28:31 PM]] [[step      143]]     [[train]]     loss: 185.31326294     [[val]]     loss: 165.22343445\n",
      "[[08/28/2017 03:28:32 PM]] [[step      144]]     [[train]]     loss: 125.19273376     [[val]]     loss: 79.58180237 \n",
      "[[08/28/2017 03:28:34 PM]] [[step      145]]     [[train]]     loss: 145.41796875     [[val]]     loss: 195.03622437\n",
      "[[08/28/2017 03:28:36 PM]] [[step      146]]     [[train]]     loss: 139.0116272      [[val]]     loss: 70.18651581 \n",
      "[[08/28/2017 03:28:38 PM]] [[step      147]]     [[train]]     loss: 172.12625122     [[val]]     loss: 119.00974274\n",
      "[[08/28/2017 03:28:39 PM]] [[step      148]]     [[train]]     loss: 120.54940796     [[val]]     loss: 164.23423767\n",
      "[[08/28/2017 03:28:41 PM]] [[step      149]]     [[train]]     loss: 85.27825165      [[val]]     loss: 66.96100616 \n",
      "[[08/28/2017 03:28:43 PM]] [[step      150]]     [[train]]     loss: 174.21420288     [[val]]     loss: 14.31982327 \n",
      "[[08/28/2017 03:28:43 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/28/2017 03:28:46 PM]] [[step      151]]     [[train]]     loss: 140.09094238     [[val]]     loss: 100.84562683\n",
      "[[08/28/2017 03:28:47 PM]] [[step      152]]     [[train]]     loss: 156.65597534     [[val]]     loss: 152.3886261 \n",
      "[[08/28/2017 03:28:49 PM]] [[step      153]]     [[train]]     loss: 78.29272461      [[val]]     loss: 27.15448761 \n",
      "[[08/28/2017 03:28:51 PM]] [[step      154]]     [[train]]     loss: 185.71791077     [[val]]     loss: 198.24467468\n",
      "[[08/28/2017 03:28:53 PM]] [[step      155]]     [[train]]     loss: 83.64425659      [[val]]     loss: 60.5623703  \n",
      "[[08/28/2017 03:28:54 PM]] [[step      156]]     [[train]]     loss: 126.15744781     [[val]]     loss: 64.35988617 \n",
      "[[08/28/2017 03:28:56 PM]] [[step      157]]     [[train]]     loss: 39.6875          [[val]]     loss: 44.40140152 \n",
      "[[08/28/2017 03:28:58 PM]] [[step      158]]     [[train]]     loss: 196.61369324     [[val]]     loss: 193.77813721\n",
      "[[08/28/2017 03:29:00 PM]] [[step      159]]     [[train]]     loss: 35.09666443      [[val]]     loss: 93.65782928 \n",
      "[[08/28/2017 03:29:01 PM]] [[step      160]]     [[train]]     loss: 154.86050415     [[val]]     loss: 58.04310989 \n",
      "[[08/28/2017 03:29:03 PM]] [[step      161]]     [[train]]     loss: 164.52223206     [[val]]     loss: 72.44015503 \n",
      "[[08/28/2017 03:29:05 PM]] [[step      162]]     [[train]]     loss: 119.71278381     [[val]]     loss: 26.81954193 \n",
      "[[08/28/2017 03:29:07 PM]] [[step      163]]     [[train]]     loss: 106.28594208     [[val]]     loss: 83.65349579 \n",
      "[[08/28/2017 03:29:08 PM]] [[step      164]]     [[train]]     loss: 134.27206421     [[val]]     loss: 173.01448059\n",
      "[[08/28/2017 03:29:10 PM]] [[step      165]]     [[train]]     loss: 186.12944031     [[val]]     loss: 165.93336487\n",
      "[[08/28/2017 03:29:12 PM]] [[step      166]]     [[train]]     loss: 41.78768539      [[val]]     loss: 93.55464935 \n",
      "[[08/28/2017 03:29:14 PM]] [[step      167]]     [[train]]     loss: 80.41078186      [[val]]     loss: 129.81661987\n",
      "[[08/28/2017 03:29:15 PM]] [[step      168]]     [[train]]     loss: 75.88703918      [[val]]     loss: 147.9833374 \n",
      "[[08/28/2017 03:29:17 PM]] [[step      169]]     [[train]]     loss: 28.68823624      [[val]]     loss: 145.70054626\n",
      "[[08/28/2017 03:29:19 PM]] [[step      170]]     [[train]]     loss: 188.81359863     [[val]]     loss: 146.94265747\n",
      "[[08/28/2017 03:29:21 PM]] [[step      171]]     [[train]]     loss: 143.89707947     [[val]]     loss: 186.56539917\n",
      "[[08/28/2017 03:29:22 PM]] [[step      172]]     [[train]]     loss: 193.98731995     [[val]]     loss: 170.3245697 \n",
      "[[08/28/2017 03:29:24 PM]] [[step      173]]     [[train]]     loss: 135.27856445     [[val]]     loss: 142.48080444\n",
      "[[08/28/2017 03:29:26 PM]] [[step      174]]     [[train]]     loss: 70.08592224      [[val]]     loss: 161.88609314\n",
      "[[08/28/2017 03:29:28 PM]] [[step      175]]     [[train]]     loss: 164.63337708     [[val]]     loss: 35.55978775 \n",
      "[[08/28/2017 03:29:29 PM]] [[step      176]]     [[train]]     loss: 196.67088318     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:29:31 PM]] [[step      177]]     [[train]]     loss: 121.06715393     [[val]]     loss: 187.3870697 \n",
      "[[08/28/2017 03:29:33 PM]] [[step      178]]     [[train]]     loss: 68.71440125      [[val]]     loss: 59.01241302 \n",
      "[[08/28/2017 03:29:35 PM]] [[step      179]]     [[train]]     loss: 140.84739685     [[val]]     loss: 57.68400955 \n",
      "[[08/28/2017 03:29:36 PM]] [[step      180]]     [[train]]     loss: 71.50862885      [[val]]     loss: 66.03723907 \n",
      "[[08/28/2017 03:29:38 PM]] [[step      181]]     [[train]]     loss: 125.50031281     [[val]]     loss: 182.05000305\n",
      "[[08/28/2017 03:29:40 PM]] [[step      182]]     [[train]]     loss: 170.52302551     [[val]]     loss: 57.36996078 \n",
      "[[08/28/2017 03:29:42 PM]] [[step      183]]     [[train]]     loss: 108.25580597     [[val]]     loss: 188.32862854\n",
      "[[08/28/2017 03:29:44 PM]] [[step      184]]     [[train]]     loss: 105.02462769     [[val]]     loss: 40.85232925 \n",
      "[[08/28/2017 03:29:45 PM]] [[step      185]]     [[train]]     loss: 177.28564453     [[val]]     loss: 188.29443359\n",
      "[[08/28/2017 03:29:47 PM]] [[step      186]]     [[train]]     loss: 178.44140625     [[val]]     loss: 58.97932816 \n",
      "[[08/28/2017 03:29:49 PM]] [[step      187]]     [[train]]     loss: 129.89855957     [[val]]     loss: 192.76278687\n",
      "[[08/28/2017 03:29:51 PM]] [[step      188]]     [[train]]     loss: 147.98497009     [[val]]     loss: 182.86940002\n",
      "[[08/28/2017 03:29:52 PM]] [[step      189]]     [[train]]     loss: 176.51795959     [[val]]     loss: 94.50274658 \n",
      "[[08/28/2017 03:29:54 PM]] [[step      190]]     [[train]]     loss: 161.96221924     [[val]]     loss: 173.62507629\n",
      "[[08/28/2017 03:29:56 PM]] [[step      191]]     [[train]]     loss: 159.09063721     [[val]]     loss: 69.81388855 \n",
      "[[08/28/2017 03:29:58 PM]] [[step      192]]     [[train]]     loss: 161.15441895     [[val]]     loss: 85.11091614 \n",
      "[[08/28/2017 03:29:59 PM]] [[step      193]]     [[train]]     loss: 190.87788391     [[val]]     loss: 194.13609314\n",
      "[[08/28/2017 03:30:01 PM]] [[step      194]]     [[train]]     loss: 186.14254761     [[val]]     loss: 41.4824791  \n",
      "[[08/28/2017 03:30:03 PM]] [[step      195]]     [[train]]     loss: 120.05207825     [[val]]     loss: 147.56503296\n",
      "[[08/28/2017 03:30:05 PM]] [[step      196]]     [[train]]     loss: 169.14941406     [[val]]     loss: 173.19508362\n",
      "[[08/28/2017 03:30:07 PM]] [[step      197]]     [[train]]     loss: 182.46580505     [[val]]     loss: 196.38069153\n",
      "[[08/28/2017 03:30:09 PM]] [[step      198]]     [[train]]     loss: 180.61541748     [[val]]     loss: 177.98432922\n",
      "[[08/28/2017 03:30:10 PM]] [[step      199]]     [[train]]     loss: 57.97785568      [[val]]     loss: 186.57087708\n",
      "[[08/28/2017 03:30:12 PM]] [[step      200]]     [[train]]     loss: 176.86172485     [[val]]     loss: 168.11886597\n",
      "[[08/28/2017 03:30:14 PM]] [[step      201]]     [[train]]     loss: 124.84327698     [[val]]     loss: 119.61251831\n",
      "[[08/28/2017 03:30:16 PM]] [[step      202]]     [[train]]     loss: 165.50712585     [[val]]     loss: 36.580513   \n",
      "[[08/28/2017 03:30:18 PM]] [[step      203]]     [[train]]     loss: 180.44476318     [[val]]     loss: 175.70834351\n",
      "[[08/28/2017 03:30:19 PM]] [[step      204]]     [[train]]     loss: 188.25682068     [[val]]     loss: 124.38140869\n",
      "[[08/28/2017 03:30:21 PM]] [[step      205]]     [[train]]     loss: 173.40235901     [[val]]     loss: 143.6547699 \n",
      "[[08/28/2017 03:30:23 PM]] [[step      206]]     [[train]]     loss: 167.82675171     [[val]]     loss: 36.919384   \n",
      "[[08/28/2017 03:30:25 PM]] [[step      207]]     [[train]]     loss: 196.39527893     [[val]]     loss: 145.74531555\n",
      "[[08/28/2017 03:30:27 PM]] [[step      208]]     [[train]]     loss: 146.73370361     [[val]]     loss: 140.66143799\n",
      "[[08/28/2017 03:30:29 PM]] [[step      209]]     [[train]]     loss: 135.54695129     [[val]]     loss: 149.06687927\n",
      "[[08/28/2017 03:30:31 PM]] [[step      210]]     [[train]]     loss: 54.25389481      [[val]]     loss: 118.41598511\n",
      "[[08/28/2017 03:30:33 PM]] [[step      211]]     [[train]]     loss: 161.69871521     [[val]]     loss: 166.858078  \n",
      "[[08/28/2017 03:30:35 PM]] [[step      212]]     [[train]]     loss: 40.79273224      [[val]]     loss: 168.64108276\n",
      "[[08/28/2017 03:30:36 PM]] [[step      213]]     [[train]]     loss: 151.64628601     [[val]]     loss: 150.50396729\n",
      "[[08/28/2017 03:30:38 PM]] [[step      214]]     [[train]]     loss: 149.01435852     [[val]]     loss: 150.5486908 \n",
      "[[08/28/2017 03:30:40 PM]] [[step      215]]     [[train]]     loss: 98.99425507      [[val]]     loss: 198.32183838\n",
      "[[08/28/2017 03:30:42 PM]] [[step      216]]     [[train]]     loss: 56.66114044      [[val]]     loss: 107.59542847\n",
      "[[08/28/2017 03:30:44 PM]] [[step      217]]     [[train]]     loss: 150.81637573     [[val]]     loss: 139.71357727\n",
      "[[08/28/2017 03:30:46 PM]] [[step      218]]     [[train]]     loss: 104.03808594     [[val]]     loss: 153.58425903\n",
      "[[08/28/2017 03:30:48 PM]] [[step      219]]     [[train]]     loss: 173.30172729     [[val]]     loss: 184.13943481\n",
      "[[08/28/2017 03:30:49 PM]] [[step      220]]     [[train]]     loss: 59.17364502      [[val]]     loss: 198.81774902\n",
      "[[08/28/2017 03:30:51 PM]] [[step      221]]     [[train]]     loss: 186.82281494     [[val]]     loss: 183.37953186\n",
      "[[08/28/2017 03:30:53 PM]] [[step      222]]     [[train]]     loss: 161.61201477     [[val]]     loss: 45.64995193 \n",
      "[[08/28/2017 03:30:55 PM]] [[step      223]]     [[train]]     loss: 163.61332703     [[val]]     loss: 137.22645569\n",
      "[[08/28/2017 03:30:57 PM]] [[step      224]]     [[train]]     loss: 195.27471924     [[val]]     loss: 182.46662903\n",
      "[[08/28/2017 03:30:59 PM]] [[step      225]]     [[train]]     loss: 111.34535217     [[val]]     loss: 159.47329712\n",
      "[[08/28/2017 03:31:00 PM]] [[step      226]]     [[train]]     loss: 154.37272644     [[val]]     loss: 22.75909805 \n",
      "[[08/28/2017 03:31:02 PM]] [[step      227]]     [[train]]     loss: 146.54708862     [[val]]     loss: 87.35424042 \n",
      "[[08/28/2017 03:31:04 PM]] [[step      228]]     [[train]]     loss: 198.9150238      [[val]]     loss: 181.74502563\n",
      "[[08/28/2017 03:31:06 PM]] [[step      229]]     [[train]]     loss: 52.5872345       [[val]]     loss: 163.50585938\n",
      "[[08/28/2017 03:31:08 PM]] [[step      230]]     [[train]]     loss: 44.99414825      [[val]]     loss: 82.04483795 \n",
      "[[08/28/2017 03:31:10 PM]] [[step      231]]     [[train]]     loss: 196.6632843      [[val]]     loss: 180.93029785\n",
      "[[08/28/2017 03:31:11 PM]] [[step      232]]     [[train]]     loss: 51.30997849      [[val]]     loss: 165.47531128\n",
      "[[08/28/2017 03:31:13 PM]] [[step      233]]     [[train]]     loss: 133.19360352     [[val]]     loss: 154.12437439\n",
      "[[08/28/2017 03:31:15 PM]] [[step      234]]     [[train]]     loss: 150.7897644      [[val]]     loss: 183.29008484\n",
      "[[08/28/2017 03:31:17 PM]] [[step      235]]     [[train]]     loss: 198.66778564     [[val]]     loss: 176.49794006\n",
      "[[08/28/2017 03:31:19 PM]] [[step      236]]     [[train]]     loss: 103.49497986     [[val]]     loss: 46.63283157 \n",
      "[[08/28/2017 03:31:21 PM]] [[step      237]]     [[train]]     loss: 131.51504517     [[val]]     loss: 78.47904205 \n",
      "[[08/28/2017 03:31:22 PM]] [[step      238]]     [[train]]     loss: 161.72827148     [[val]]     loss: 181.06309509\n",
      "[[08/28/2017 03:31:24 PM]] [[step      239]]     [[train]]     loss: 184.91693115     [[val]]     loss: 138.57473755\n",
      "[[08/28/2017 03:31:26 PM]] [[step      240]]     [[train]]     loss: 156.73127747     [[val]]     loss: 145.84634399\n",
      "[[08/28/2017 03:31:28 PM]] [[step      241]]     [[train]]     loss: 104.45135498     [[val]]     loss: 32.97599411 \n",
      "[[08/28/2017 03:31:30 PM]] [[step      242]]     [[train]]     loss: 162.42755127     [[val]]     loss: 65.52768707 \n",
      "[[08/28/2017 03:31:32 PM]] [[step      243]]     [[train]]     loss: 54.65171432      [[val]]     loss: 59.15629196 \n",
      "[[08/28/2017 03:31:33 PM]] [[step      244]]     [[train]]     loss: 92.82899475      [[val]]     loss: 183.23835754\n",
      "[[08/28/2017 03:31:35 PM]] [[step      245]]     [[train]]     loss: 176.1796875      [[val]]     loss: 156.27662659\n",
      "[[08/28/2017 03:31:37 PM]] [[step      246]]     [[train]]     loss: 187.46029663     [[val]]     loss: 186.23840332\n",
      "[[08/28/2017 03:31:39 PM]] [[step      247]]     [[train]]     loss: 121.98282623     [[val]]     loss: 88.735672   \n",
      "[[08/28/2017 03:31:41 PM]] [[step      248]]     [[train]]     loss: 67.84280396      [[val]]     loss: 194.55006409\n",
      "[[08/28/2017 03:31:43 PM]] [[step      249]]     [[train]]     loss: 194.72851562     [[val]]     loss: 86.06363678 \n",
      "[[08/28/2017 03:31:45 PM]] [[step      250]]     [[train]]     loss: 76.11016846      [[val]]     loss: 154.51501465\n",
      "[[08/28/2017 03:31:46 PM]] [[step      251]]     [[train]]     loss: 96.86458588      [[val]]     loss: 145.92047119\n",
      "[[08/28/2017 03:31:48 PM]] [[step      252]]     [[train]]     loss: 70.03068542      [[val]]     loss: 183.57693481\n",
      "[[08/28/2017 03:31:50 PM]] [[step      253]]     [[train]]     loss: 146.16844177     [[val]]     loss: 115.50004578\n",
      "[[08/28/2017 03:31:52 PM]] [[step      254]]     [[train]]     loss: 153.92405701     [[val]]     loss: 136.20849609\n",
      "[[08/28/2017 03:31:54 PM]] [[step      255]]     [[train]]     loss: 125.64074707     [[val]]     loss: 63.56938934 \n",
      "[[08/28/2017 03:31:56 PM]] [[step      256]]     [[train]]     loss: 74.06200409      [[val]]     loss: 95.59693146 \n",
      "[[08/28/2017 03:31:57 PM]] [[step      257]]     [[train]]     loss: 95.39601898      [[val]]     loss: 123.27232361\n",
      "[[08/28/2017 03:31:59 PM]] [[step      258]]     [[train]]     loss: 82.82821655      [[val]]     loss: 42.23078156 \n",
      "[[08/28/2017 03:32:01 PM]] [[step      259]]     [[train]]     loss: 100.76399994     [[val]]     loss: 130.65023804\n",
      "[[08/28/2017 03:32:03 PM]] [[step      260]]     [[train]]     loss: 59.01466751      [[val]]     loss: 77.34537506 \n",
      "[[08/28/2017 03:32:05 PM]] [[step      261]]     [[train]]     loss: 188.83692932     [[val]]     loss: 177.88726807\n",
      "[[08/28/2017 03:32:07 PM]] [[step      262]]     [[train]]     loss: 69.59957123      [[val]]     loss: 59.39985657 \n",
      "[[08/28/2017 03:32:08 PM]] [[step      263]]     [[train]]     loss: 129.54336548     [[val]]     loss: 167.04652405\n",
      "[[08/28/2017 03:32:10 PM]] [[step      264]]     [[train]]     loss: 42.54353333      [[val]]     loss: 144.41767883\n",
      "[[08/28/2017 03:32:12 PM]] [[step      265]]     [[train]]     loss: 191.13432312     [[val]]     loss: 158.86218262\n",
      "[[08/28/2017 03:32:14 PM]] [[step      266]]     [[train]]     loss: 73.17900085      [[val]]     loss: 158.54045105\n",
      "[[08/28/2017 03:32:16 PM]] [[step      267]]     [[train]]     loss: 197.29304504     [[val]]     loss: 146.68952942\n",
      "[[08/28/2017 03:32:18 PM]] [[step      268]]     [[train]]     loss: 139.76170349     [[val]]     loss: 54.37391281 \n",
      "[[08/28/2017 03:32:19 PM]] [[step      269]]     [[train]]     loss: 178.02664185     [[val]]     loss: 128.06803894\n",
      "[[08/28/2017 03:32:21 PM]] [[step      270]]     [[train]]     loss: 181.78479004     [[val]]     loss: 158.50166321\n",
      "[[08/28/2017 03:32:23 PM]] [[step      271]]     [[train]]     loss: 176.89118958     [[val]]     loss: 197.66720581\n",
      "[[08/28/2017 03:32:25 PM]] [[step      272]]     [[train]]     loss: 190.3057251      [[val]]     loss: 189.5965271 \n",
      "[[08/28/2017 03:32:27 PM]] [[step      273]]     [[train]]     loss: 134.70935059     [[val]]     loss: 81.712677   \n",
      "[[08/28/2017 03:32:29 PM]] [[step      274]]     [[train]]     loss: 186.22969055     [[val]]     loss: 79.48472595 \n",
      "[[08/28/2017 03:32:30 PM]] [[step      275]]     [[train]]     loss: 200.0            [[val]]     loss: 148.90319824\n",
      "[[08/28/2017 03:32:32 PM]] [[step      276]]     [[train]]     loss: 140.48500061     [[val]]     loss: 75.47799683 \n",
      "[[08/28/2017 03:32:34 PM]] [[step      277]]     [[train]]     loss: 138.89479065     [[val]]     loss: 64.66497803 \n",
      "[[08/28/2017 03:32:36 PM]] [[step      278]]     [[train]]     loss: 117.94660187     [[val]]     loss: 117.85186768\n",
      "[[08/28/2017 03:32:38 PM]] [[step      279]]     [[train]]     loss: 183.80033875     [[val]]     loss: 160.04847717\n",
      "[[08/28/2017 03:32:40 PM]] [[step      280]]     [[train]]     loss: 79.14678955      [[val]]     loss: 192.92625427\n",
      "[[08/28/2017 03:32:41 PM]] [[step      281]]     [[train]]     loss: 52.32966614      [[val]]     loss: 164.91452026\n",
      "[[08/28/2017 03:32:43 PM]] [[step      282]]     [[train]]     loss: 180.07073975     [[val]]     loss: 138.55784607\n",
      "[[08/28/2017 03:32:45 PM]] [[step      283]]     [[train]]     loss: 191.73379517     [[val]]     loss: 111.55974579\n",
      "[[08/28/2017 03:32:47 PM]] [[step      284]]     [[train]]     loss: 113.23506927     [[val]]     loss: 102.97008514\n",
      "[[08/28/2017 03:32:49 PM]] [[step      285]]     [[train]]     loss: 168.71136475     [[val]]     loss: 157.13040161\n",
      "[[08/28/2017 03:32:51 PM]] [[step      286]]     [[train]]     loss: 38.46216965      [[val]]     loss: 196.50527954\n",
      "[[08/28/2017 03:32:52 PM]] [[step      287]]     [[train]]     loss: 140.89775085     [[val]]     loss: 73.22683716 \n",
      "[[08/28/2017 03:32:54 PM]] [[step      288]]     [[train]]     loss: 145.75302124     [[val]]     loss: 165.50088501\n",
      "[[08/28/2017 03:32:56 PM]] [[step      289]]     [[train]]     loss: 74.68250275      [[val]]     loss: 155.39395142\n",
      "[[08/28/2017 03:32:58 PM]] [[step      290]]     [[train]]     loss: 150.19833374     [[val]]     loss: 172.07003784\n",
      "[[08/28/2017 03:33:00 PM]] [[step      291]]     [[train]]     loss: 84.34853363      [[val]]     loss: 181.3260498 \n",
      "[[08/28/2017 03:33:02 PM]] [[step      292]]     [[train]]     loss: 76.9756012       [[val]]     loss: 67.3065567  \n",
      "[[08/28/2017 03:33:03 PM]] [[step      293]]     [[train]]     loss: 173.54319763     [[val]]     loss: 161.54376221\n",
      "[[08/28/2017 03:33:05 PM]] [[step      294]]     [[train]]     loss: 101.85600281     [[val]]     loss: 35.45057297 \n",
      "[[08/28/2017 03:33:07 PM]] [[step      295]]     [[train]]     loss: 43.04541779      [[val]]     loss: 199.5509491 \n",
      "[[08/28/2017 03:33:09 PM]] [[step      296]]     [[train]]     loss: 195.02999878     [[val]]     loss: 133.8984375 \n",
      "[[08/28/2017 03:33:11 PM]] [[step      297]]     [[train]]     loss: 194.59634399     [[val]]     loss: 172.01646423\n",
      "[[08/28/2017 03:33:13 PM]] [[step      298]]     [[train]]     loss: 154.54045105     [[val]]     loss: 125.17459106\n",
      "[[08/28/2017 03:33:14 PM]] [[step      299]]     [[train]]     loss: 109.28335571     [[val]]     loss: 62.30192947 \n",
      "[[08/28/2017 03:33:16 PM]] [[step      300]]     [[train]]     loss: 173.43074036     [[val]]     loss: 181.44929504\n",
      "[[08/28/2017 03:33:18 PM]] [[step      301]]     [[train]]     loss: 105.9138031      [[val]]     loss: 134.47944641\n",
      "[[08/28/2017 03:33:20 PM]] [[step      302]]     [[train]]     loss: 181.78964233     [[val]]     loss: 169.9727478 \n",
      "[[08/28/2017 03:33:22 PM]] [[step      303]]     [[train]]     loss: 181.02742004     [[val]]     loss: 163.45417786\n",
      "[[08/28/2017 03:33:24 PM]] [[step      304]]     [[train]]     loss: 68.86794281      [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:33:25 PM]] [[step      305]]     [[train]]     loss: 48.9681778       [[val]]     loss: 178.52035522\n",
      "[[08/28/2017 03:33:27 PM]] [[step      306]]     [[train]]     loss: 127.89204407     [[val]]     loss: 47.4753685  \n",
      "[[08/28/2017 03:33:29 PM]] [[step      307]]     [[train]]     loss: 176.06759644     [[val]]     loss: 106.52631378\n",
      "[[08/28/2017 03:33:31 PM]] [[step      308]]     [[train]]     loss: 167.26753235     [[val]]     loss: 90.09375763 \n",
      "[[08/28/2017 03:33:33 PM]] [[step      309]]     [[train]]     loss: 122.60816956     [[val]]     loss: 131.0506897 \n",
      "[[08/28/2017 03:33:35 PM]] [[step      310]]     [[train]]     loss: 184.67982483     [[val]]     loss: 164.87315369\n",
      "[[08/28/2017 03:33:37 PM]] [[step      311]]     [[train]]     loss: 70.22290039      [[val]]     loss: 191.74952698\n",
      "[[08/28/2017 03:33:38 PM]] [[step      312]]     [[train]]     loss: 195.50053406     [[val]]     loss: 122.34539032\n",
      "[[08/28/2017 03:33:40 PM]] [[step      313]]     [[train]]     loss: 127.49568176     [[val]]     loss: 43.1956749  \n",
      "[[08/28/2017 03:33:42 PM]] [[step      314]]     [[train]]     loss: 26.13431168      [[val]]     loss: 112.39460754\n",
      "[[08/28/2017 03:33:44 PM]] [[step      315]]     [[train]]     loss: 180.60971069     [[val]]     loss: 91.43452454 \n",
      "[[08/28/2017 03:33:46 PM]] [[step      316]]     [[train]]     loss: 189.00125122     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:33:48 PM]] [[step      317]]     [[train]]     loss: 158.36004639     [[val]]     loss: 101.79008484\n",
      "[[08/28/2017 03:33:49 PM]] [[step      318]]     [[train]]     loss: 90.80286407      [[val]]     loss: 126.73513031\n",
      "[[08/28/2017 03:33:51 PM]] [[step      319]]     [[train]]     loss: 165.62036133     [[val]]     loss: 45.48479462 \n",
      "[[08/28/2017 03:33:53 PM]] [[step      320]]     [[train]]     loss: 172.08378601     [[val]]     loss: 198.82289124\n",
      "[[08/28/2017 03:33:55 PM]] [[step      321]]     [[train]]     loss: 48.49306488      [[val]]     loss: 116.88092804\n",
      "[[08/28/2017 03:33:57 PM]] [[step      322]]     [[train]]     loss: 131.31880188     [[val]]     loss: 169.17449951\n",
      "[[08/28/2017 03:33:58 PM]] [[step      323]]     [[train]]     loss: 191.33560181     [[val]]     loss: 130.4103241 \n",
      "[[08/28/2017 03:34:00 PM]] [[step      324]]     [[train]]     loss: 35.11663818      [[val]]     loss: 175.83338928\n",
      "[[08/28/2017 03:34:02 PM]] [[step      325]]     [[train]]     loss: 48.51092529      [[val]]     loss: 78.55685425 \n",
      "[[08/28/2017 03:34:04 PM]] [[step      326]]     [[train]]     loss: 200.0            [[val]]     loss: 128.5209198 \n",
      "[[08/28/2017 03:34:06 PM]] [[step      327]]     [[train]]     loss: 119.38981628     [[val]]     loss: 163.22137451\n",
      "[[08/28/2017 03:34:08 PM]] [[step      328]]     [[train]]     loss: 190.14001465     [[val]]     loss: 160.33903503\n",
      "[[08/28/2017 03:34:09 PM]] [[step      329]]     [[train]]     loss: 123.45394135     [[val]]     loss: 149.6350708 \n",
      "[[08/28/2017 03:34:11 PM]] [[step      330]]     [[train]]     loss: 174.48529053     [[val]]     loss: 139.89193726\n",
      "[[08/28/2017 03:34:13 PM]] [[step      331]]     [[train]]     loss: 165.51426697     [[val]]     loss: 158.70674133\n",
      "[[08/28/2017 03:34:15 PM]] [[step      332]]     [[train]]     loss: 28.3819294       [[val]]     loss: 194.63264465\n",
      "[[08/28/2017 03:34:17 PM]] [[step      333]]     [[train]]     loss: 138.41123962     [[val]]     loss: 111.97793579\n",
      "[[08/28/2017 03:34:19 PM]] [[step      334]]     [[train]]     loss: 47.76019669      [[val]]     loss: 65.22822571 \n",
      "[[08/28/2017 03:34:20 PM]] [[step      335]]     [[train]]     loss: 47.43736267      [[val]]     loss: 84.56479645 \n",
      "[[08/28/2017 03:34:22 PM]] [[step      336]]     [[train]]     loss: 157.37609863     [[val]]     loss: 182.3006897 \n",
      "[[08/28/2017 03:34:24 PM]] [[step      337]]     [[train]]     loss: 181.5509491      [[val]]     loss: 108.22240448\n",
      "[[08/28/2017 03:34:26 PM]] [[step      338]]     [[train]]     loss: 42.48958969      [[val]]     loss: 194.54692078\n",
      "[[08/28/2017 03:34:28 PM]] [[step      339]]     [[train]]     loss: 92.2950058       [[val]]     loss: 184.8203125 \n",
      "[[08/28/2017 03:34:30 PM]] [[step      340]]     [[train]]     loss: 63.57826996      [[val]]     loss: 190.78486633\n",
      "[[08/28/2017 03:34:31 PM]] [[step      341]]     [[train]]     loss: 134.42797852     [[val]]     loss: 57.10795593 \n",
      "[[08/28/2017 03:34:33 PM]] [[step      342]]     [[train]]     loss: 67.61910248      [[val]]     loss: 48.80653763 \n",
      "[[08/28/2017 03:34:35 PM]] [[step      343]]     [[train]]     loss: 171.88479614     [[val]]     loss: 121.90569305\n",
      "[[08/28/2017 03:34:37 PM]] [[step      344]]     [[train]]     loss: 80.5417099       [[val]]     loss: 133.545578  \n",
      "[[08/28/2017 03:34:39 PM]] [[step      345]]     [[train]]     loss: 198.78759766     [[val]]     loss: 182.16743469\n",
      "[[08/28/2017 03:34:41 PM]] [[step      346]]     [[train]]     loss: 157.35751343     [[val]]     loss: 118.47212219\n",
      "[[08/28/2017 03:34:43 PM]] [[step      347]]     [[train]]     loss: 199.35055542     [[val]]     loss: 76.24474335 \n",
      "[[08/28/2017 03:34:44 PM]] [[step      348]]     [[train]]     loss: 174.21795654     [[val]]     loss: 67.54238892 \n",
      "[[08/28/2017 03:34:46 PM]] [[step      349]]     [[train]]     loss: 156.54519653     [[val]]     loss: 107.15211487\n",
      "[[08/28/2017 03:34:48 PM]] [[step      350]]     [[train]]     loss: 85.32826233      [[val]]     loss: 54.17041779 \n",
      "[[08/28/2017 03:34:50 PM]] [[step      351]]     [[train]]     loss: 136.48962402     [[val]]     loss: 183.92462158\n",
      "[[08/28/2017 03:34:52 PM]] [[step      352]]     [[train]]     loss: 57.07577896      [[val]]     loss: 114.40718079\n",
      "[[08/28/2017 03:34:53 PM]] [[step      353]]     [[train]]     loss: 47.88355637      [[val]]     loss: 106.84888458\n",
      "[[08/28/2017 03:34:55 PM]] [[step      354]]     [[train]]     loss: 79.38214874      [[val]]     loss: 58.37294388 \n",
      "[[08/28/2017 03:34:57 PM]] [[step      355]]     [[train]]     loss: 110.88032532     [[val]]     loss: 160.40519714\n",
      "[[08/28/2017 03:34:59 PM]] [[step      356]]     [[train]]     loss: 117.32069397     [[val]]     loss: 39.10444641 \n",
      "[[08/28/2017 03:35:01 PM]] [[step      357]]     [[train]]     loss: 184.35661316     [[val]]     loss: 173.30599976\n",
      "[[08/28/2017 03:35:03 PM]] [[step      358]]     [[train]]     loss: 37.13086319      [[val]]     loss: 155.16703796\n",
      "[[08/28/2017 03:35:04 PM]] [[step      359]]     [[train]]     loss: 192.05128479     [[val]]     loss: 67.93625641 \n",
      "[[08/28/2017 03:35:06 PM]] [[step      360]]     [[train]]     loss: 181.77503967     [[val]]     loss: 194.25727844\n",
      "[[08/28/2017 03:35:08 PM]] [[step      361]]     [[train]]     loss: 163.8129425      [[val]]     loss: 158.79397583\n",
      "[[08/28/2017 03:35:10 PM]] [[step      362]]     [[train]]     loss: 123.71080017     [[val]]     loss: 78.50600433 \n",
      "[[08/28/2017 03:35:12 PM]] [[step      363]]     [[train]]     loss: 190.78507996     [[val]]     loss: 195.63612366\n",
      "[[08/28/2017 03:35:14 PM]] [[step      364]]     [[train]]     loss: 188.51196289     [[val]]     loss: 108.72955322\n",
      "[[08/28/2017 03:35:15 PM]] [[step      365]]     [[train]]     loss: 193.89637756     [[val]]     loss: 115.32164764\n",
      "[[08/28/2017 03:35:17 PM]] [[step      366]]     [[train]]     loss: 150.06887817     [[val]]     loss: 54.55152893 \n",
      "[[08/28/2017 03:35:19 PM]] [[step      367]]     [[train]]     loss: 85.69552612      [[val]]     loss: 88.92340851 \n",
      "[[08/28/2017 03:35:21 PM]] [[step      368]]     [[train]]     loss: 155.37960815     [[val]]     loss: 33.84188461 \n",
      "[[08/28/2017 03:35:23 PM]] [[step      369]]     [[train]]     loss: 50.10012436      [[val]]     loss: 139.46110535\n",
      "[[08/28/2017 03:35:25 PM]] [[step      370]]     [[train]]     loss: 188.858078       [[val]]     loss: 180.27500916\n",
      "[[08/28/2017 03:35:27 PM]] [[step      371]]     [[train]]     loss: 179.6363678      [[val]]     loss: 86.9704361  \n",
      "[[08/28/2017 03:35:28 PM]] [[step      372]]     [[train]]     loss: 77.29419708      [[val]]     loss: 171.37892151\n",
      "[[08/28/2017 03:35:30 PM]] [[step      373]]     [[train]]     loss: 198.35954285     [[val]]     loss: 153.08148193\n",
      "[[08/28/2017 03:35:32 PM]] [[step      374]]     [[train]]     loss: 79.36464691      [[val]]     loss: 139.15722656\n",
      "[[08/28/2017 03:35:34 PM]] [[step      375]]     [[train]]     loss: 195.84135437     [[val]]     loss: 82.63076019 \n",
      "[[08/28/2017 03:35:36 PM]] [[step      376]]     [[train]]     loss: 166.84794617     [[val]]     loss: 117.16628265\n",
      "[[08/28/2017 03:35:37 PM]] [[step      377]]     [[train]]     loss: 48.87432861      [[val]]     loss: 108.45662689\n",
      "[[08/28/2017 03:35:39 PM]] [[step      378]]     [[train]]     loss: 144.3381958      [[val]]     loss: 112.6332016 \n",
      "[[08/28/2017 03:35:41 PM]] [[step      379]]     [[train]]     loss: 111.7133255      [[val]]     loss: 106.45230865\n",
      "[[08/28/2017 03:35:43 PM]] [[step      380]]     [[train]]     loss: 191.2071228      [[val]]     loss: 199.61853027\n",
      "[[08/28/2017 03:35:45 PM]] [[step      381]]     [[train]]     loss: 115.39024353     [[val]]     loss: 175.46574402\n",
      "[[08/28/2017 03:35:47 PM]] [[step      382]]     [[train]]     loss: 64.88512421      [[val]]     loss: 25.72957039 \n",
      "[[08/28/2017 03:35:48 PM]] [[step      383]]     [[train]]     loss: 185.31216431     [[val]]     loss: 43.32434845 \n",
      "[[08/28/2017 03:35:50 PM]] [[step      384]]     [[train]]     loss: 90.47699738      [[val]]     loss: 169.22288513\n",
      "[[08/28/2017 03:35:52 PM]] [[step      385]]     [[train]]     loss: 183.02392578     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:35:54 PM]] [[step      386]]     [[train]]     loss: 52.03926468      [[val]]     loss: 78.68524933 \n",
      "[[08/28/2017 03:35:56 PM]] [[step      387]]     [[train]]     loss: 192.16671753     [[val]]     loss: 64.74407959 \n",
      "[[08/28/2017 03:35:58 PM]] [[step      388]]     [[train]]     loss: 198.45959473     [[val]]     loss: 126.24646759\n",
      "[[08/28/2017 03:35:59 PM]] [[step      389]]     [[train]]     loss: 187.67658997     [[val]]     loss: 38.72330475 \n",
      "[[08/28/2017 03:36:01 PM]] [[step      390]]     [[train]]     loss: 129.27438354     [[val]]     loss: 164.56307983\n",
      "[[08/28/2017 03:36:03 PM]] [[step      391]]     [[train]]     loss: 173.07769775     [[val]]     loss: 131.94425964\n",
      "[[08/28/2017 03:36:05 PM]] [[step      392]]     [[train]]     loss: 86.66183472      [[val]]     loss: 125.53948212\n",
      "[[08/28/2017 03:36:07 PM]] [[step      393]]     [[train]]     loss: 183.01580811     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:36:09 PM]] [[step      394]]     [[train]]     loss: 176.7792511      [[val]]     loss: 149.49795532\n",
      "[[08/28/2017 03:36:10 PM]] [[step      395]]     [[train]]     loss: 153.8167572      [[val]]     loss: 143.37670898\n",
      "[[08/28/2017 03:36:12 PM]] [[step      396]]     [[train]]     loss: 118.62622833     [[val]]     loss: 165.2691803 \n",
      "[[08/28/2017 03:36:14 PM]] [[step      397]]     [[train]]     loss: 192.97842407     [[val]]     loss: 134.31958008\n",
      "[[08/28/2017 03:36:16 PM]] [[step      398]]     [[train]]     loss: 177.215271       [[val]]     loss: 71.78096008 \n",
      "[[08/28/2017 03:36:18 PM]] [[step      399]]     [[train]]     loss: 149.69110107     [[val]]     loss: 181.31069946\n",
      "[[08/28/2017 03:36:19 PM]] [[step      400]]     [[train]]     loss: 75.73184967      [[val]]     loss: 124.75850677\n",
      "[[08/28/2017 03:36:21 PM]] [[step      401]]     [[train]]     loss: 142.2061615      [[val]]     loss: 185.40019226\n",
      "[[08/28/2017 03:36:23 PM]] [[step      402]]     [[train]]     loss: 139.84353638     [[val]]     loss: 83.52526093 \n",
      "[[08/28/2017 03:36:25 PM]] [[step      403]]     [[train]]     loss: 156.15122986     [[val]]     loss: 183.95628357\n",
      "[[08/28/2017 03:36:26 PM]] [[step      404]]     [[train]]     loss: 122.30288696     [[val]]     loss: 25.80982208 \n",
      "[[08/28/2017 03:36:28 PM]] [[step      405]]     [[train]]     loss: 42.66841125      [[val]]     loss: 118.49884033\n",
      "[[08/28/2017 03:36:30 PM]] [[step      406]]     [[train]]     loss: 41.16932678      [[val]]     loss: 162.94874573\n",
      "[[08/28/2017 03:36:32 PM]] [[step      407]]     [[train]]     loss: 39.71817017      [[val]]     loss: 122.6765976 \n",
      "[[08/28/2017 03:36:34 PM]] [[step      408]]     [[train]]     loss: 59.90929031      [[val]]     loss: 187.25228882\n",
      "[[08/28/2017 03:36:35 PM]] [[step      409]]     [[train]]     loss: 190.4337616      [[val]]     loss: 29.09222031 \n",
      "[[08/28/2017 03:36:37 PM]] [[step      410]]     [[train]]     loss: 161.8848877      [[val]]     loss: 140.23033142\n",
      "[[08/28/2017 03:36:39 PM]] [[step      411]]     [[train]]     loss: 194.28398132     [[val]]     loss: 14.9019022  \n",
      "[[08/28/2017 03:36:41 PM]] [[step      412]]     [[train]]     loss: 91.17044067      [[val]]     loss: 140.22570801\n",
      "[[08/28/2017 03:36:43 PM]] [[step      413]]     [[train]]     loss: 187.3821106      [[val]]     loss: 122.82364655\n",
      "[[08/28/2017 03:36:44 PM]] [[step      414]]     [[train]]     loss: 169.0953064      [[val]]     loss: 100.50665283\n",
      "[[08/28/2017 03:36:46 PM]] [[step      415]]     [[train]]     loss: 180.58956909     [[val]]     loss: 70.72875214 \n",
      "[[08/28/2017 03:36:48 PM]] [[step      416]]     [[train]]     loss: 47.31682968      [[val]]     loss: 112.06347656\n",
      "[[08/28/2017 03:36:50 PM]] [[step      417]]     [[train]]     loss: 130.28363037     [[val]]     loss: 157.40805054\n",
      "[[08/28/2017 03:36:51 PM]] [[step      418]]     [[train]]     loss: 92.85539246      [[val]]     loss: 82.26122284 \n",
      "[[08/28/2017 03:36:53 PM]] [[step      419]]     [[train]]     loss: 195.379776       [[val]]     loss: 117.80688477\n",
      "[[08/28/2017 03:36:55 PM]] [[step      420]]     [[train]]     loss: 188.60551453     [[val]]     loss: 188.68518066\n",
      "[[08/28/2017 03:36:57 PM]] [[step      421]]     [[train]]     loss: 162.66061401     [[val]]     loss: 122.31923676\n",
      "[[08/28/2017 03:36:59 PM]] [[step      422]]     [[train]]     loss: 151.68437195     [[val]]     loss: 93.58178711 \n",
      "[[08/28/2017 03:37:00 PM]] [[step      423]]     [[train]]     loss: 189.75857544     [[val]]     loss: 83.40842438 \n",
      "[[08/28/2017 03:37:02 PM]] [[step      424]]     [[train]]     loss: 185.96099854     [[val]]     loss: 22.96935463 \n",
      "[[08/28/2017 03:37:04 PM]] [[step      425]]     [[train]]     loss: 140.40525818     [[val]]     loss: 173.34030151\n",
      "[[08/28/2017 03:37:06 PM]] [[step      426]]     [[train]]     loss: 161.33370972     [[val]]     loss: 166.42066956\n",
      "[[08/28/2017 03:37:07 PM]] [[step      427]]     [[train]]     loss: 200.0            [[val]]     loss: 83.01299286 \n",
      "[[08/28/2017 03:37:09 PM]] [[step      428]]     [[train]]     loss: 176.25114441     [[val]]     loss: 50.79001617 \n",
      "[[08/28/2017 03:37:11 PM]] [[step      429]]     [[train]]     loss: 70.95349884      [[val]]     loss: 189.82931519\n",
      "[[08/28/2017 03:37:13 PM]] [[step      430]]     [[train]]     loss: 181.81863403     [[val]]     loss: 153.12045288\n",
      "[[08/28/2017 03:37:14 PM]] [[step      431]]     [[train]]     loss: 198.37026978     [[val]]     loss: 161.15190125\n",
      "[[08/28/2017 03:37:16 PM]] [[step      432]]     [[train]]     loss: 134.02053833     [[val]]     loss: 168.72242737\n",
      "[[08/28/2017 03:37:18 PM]] [[step      433]]     [[train]]     loss: 85.72126007      [[val]]     loss: 125.34446716\n",
      "[[08/28/2017 03:37:20 PM]] [[step      434]]     [[train]]     loss: 149.56318665     [[val]]     loss: 79.97690582 \n",
      "[[08/28/2017 03:37:22 PM]] [[step      435]]     [[train]]     loss: 185.01948547     [[val]]     loss: 150.73294067\n",
      "[[08/28/2017 03:37:23 PM]] [[step      436]]     [[train]]     loss: 167.32536316     [[val]]     loss: 22.0757637  \n",
      "[[08/28/2017 03:37:25 PM]] [[step      437]]     [[train]]     loss: 122.92024231     [[val]]     loss: 131.43659973\n",
      "[[08/28/2017 03:37:27 PM]] [[step      438]]     [[train]]     loss: 137.00410461     [[val]]     loss: 174.98287964\n",
      "[[08/28/2017 03:37:29 PM]] [[step      439]]     [[train]]     loss: 88.26021576      [[val]]     loss: 146.7230835 \n",
      "[[08/28/2017 03:37:30 PM]] [[step      440]]     [[train]]     loss: 162.14411926     [[val]]     loss: 193.09881592\n",
      "[[08/28/2017 03:37:32 PM]] [[step      441]]     [[train]]     loss: 184.28471375     [[val]]     loss: 184.26930237\n",
      "[[08/28/2017 03:37:34 PM]] [[step      442]]     [[train]]     loss: 114.59136963     [[val]]     loss: 147.21986389\n",
      "[[08/28/2017 03:37:36 PM]] [[step      443]]     [[train]]     loss: 54.05059433      [[val]]     loss: 85.50810242 \n",
      "[[08/28/2017 03:37:37 PM]] [[step      444]]     [[train]]     loss: 154.09916687     [[val]]     loss: 155.92927551\n",
      "[[08/28/2017 03:37:39 PM]] [[step      445]]     [[train]]     loss: 145.94955444     [[val]]     loss: 82.73110962 \n",
      "[[08/28/2017 03:37:41 PM]] [[step      446]]     [[train]]     loss: 197.5105896      [[val]]     loss: 169.34947205\n",
      "[[08/28/2017 03:37:43 PM]] [[step      447]]     [[train]]     loss: 119.77124023     [[val]]     loss: 85.13607025 \n",
      "[[08/28/2017 03:37:44 PM]] [[step      448]]     [[train]]     loss: 142.7588501      [[val]]     loss: 82.08514404 \n",
      "[[08/28/2017 03:37:46 PM]] [[step      449]]     [[train]]     loss: 164.70744324     [[val]]     loss: 124.20066833\n",
      "[[08/28/2017 03:37:48 PM]] [[step      450]]     [[train]]     loss: 135.89442444     [[val]]     loss: 45.10066986 \n",
      "[[08/28/2017 03:37:48 PM]] \n",
      "[[08/28/2017 03:37:48 PM]] Restoring model from ./tf-data/checkpoints/model-150\n",
      "INFO:tensorflow:Restoring parameters from ./tf-data/checkpoints/model-150\n",
      "[[08/28/2017 03:37:48 PM]] Half learning rate to 0.50005 and restore step 150\n",
      "[[08/28/2017 03:37:50 PM]] [[step      151]]     [[train]]     loss: 112.09988403     [[val]]     loss: 193.32774353\n",
      "[[08/28/2017 03:37:52 PM]] [[step      152]]     [[train]]     loss: 187.67346191     [[val]]     loss: 197.28092957\n",
      "[[08/28/2017 03:37:53 PM]] [[step      153]]     [[train]]     loss: 188.97184753     [[val]]     loss: 45.15951157 \n",
      "[[08/28/2017 03:37:55 PM]] [[step      154]]     [[train]]     loss: 117.62982941     [[val]]     loss: 166.95701599\n",
      "[[08/28/2017 03:37:57 PM]] [[step      155]]     [[train]]     loss: 120.1025238      [[val]]     loss: 160.97270203\n",
      "[[08/28/2017 03:37:59 PM]] [[step      156]]     [[train]]     loss: 47.96295547      [[val]]     loss: 184.05470276\n",
      "[[08/28/2017 03:38:00 PM]] [[step      157]]     [[train]]     loss: 196.38963318     [[val]]     loss: 194.19755554\n",
      "[[08/28/2017 03:38:02 PM]] [[step      158]]     [[train]]     loss: 161.88989258     [[val]]     loss: 199.89637756\n",
      "[[08/28/2017 03:38:04 PM]] [[step      159]]     [[train]]     loss: 33.28879929      [[val]]     loss: 144.50653076\n",
      "[[08/28/2017 03:38:06 PM]] [[step      160]]     [[train]]     loss: 83.20457458      [[val]]     loss: 129.91412354\n",
      "[[08/28/2017 03:38:08 PM]] [[step      161]]     [[train]]     loss: 106.93834686     [[val]]     loss: 118.24266052\n",
      "[[08/28/2017 03:38:09 PM]] [[step      162]]     [[train]]     loss: 153.26263428     [[val]]     loss: 26.73030281 \n",
      "[[08/28/2017 03:38:11 PM]] [[step      163]]     [[train]]     loss: 195.01138306     [[val]]     loss: 159.72447205\n",
      "[[08/28/2017 03:38:13 PM]] [[step      164]]     [[train]]     loss: 195.41809082     [[val]]     loss: 183.09762573\n",
      "[[08/28/2017 03:38:15 PM]] [[step      165]]     [[train]]     loss: 139.30830383     [[val]]     loss: 171.10668945\n",
      "[[08/28/2017 03:38:17 PM]] [[step      166]]     [[train]]     loss: 124.62030792     [[val]]     loss: 180.39575195\n",
      "[[08/28/2017 03:38:19 PM]] [[step      167]]     [[train]]     loss: 46.6563797       [[val]]     loss: 193.43739319\n",
      "[[08/28/2017 03:38:20 PM]] [[step      168]]     [[train]]     loss: 42.01060104      [[val]]     loss: 30.15633011 \n",
      "[[08/28/2017 03:38:22 PM]] [[step      169]]     [[train]]     loss: 142.63642883     [[val]]     loss: 119.20249176\n",
      "[[08/28/2017 03:38:24 PM]] [[step      170]]     [[train]]     loss: 164.36325073     [[val]]     loss: 110.89568329\n",
      "[[08/28/2017 03:38:26 PM]] [[step      171]]     [[train]]     loss: 155.72012329     [[val]]     loss: 57.87723541 \n",
      "[[08/28/2017 03:38:28 PM]] [[step      172]]     [[train]]     loss: 84.71092987      [[val]]     loss: 190.47827148\n",
      "[[08/28/2017 03:38:29 PM]] [[step      173]]     [[train]]     loss: 151.12734985     [[val]]     loss: 198.81025696\n",
      "[[08/28/2017 03:38:31 PM]] [[step      174]]     [[train]]     loss: 121.79361725     [[val]]     loss: 40.47303009 \n",
      "[[08/28/2017 03:38:33 PM]] [[step      175]]     [[train]]     loss: 149.98455811     [[val]]     loss: 195.45153809\n",
      "[[08/28/2017 03:38:35 PM]] [[step      176]]     [[train]]     loss: 151.41653442     [[val]]     loss: 141.54260254\n",
      "[[08/28/2017 03:38:36 PM]] [[step      177]]     [[train]]     loss: 72.92249298      [[val]]     loss: 55.73191833 \n",
      "[[08/28/2017 03:38:38 PM]] [[step      178]]     [[train]]     loss: 96.1785965       [[val]]     loss: 173.63475037\n",
      "[[08/28/2017 03:38:40 PM]] [[step      179]]     [[train]]     loss: 197.82150269     [[val]]     loss: 141.66564941\n",
      "[[08/28/2017 03:38:42 PM]] [[step      180]]     [[train]]     loss: 186.07914734     [[val]]     loss: 162.90666199\n",
      "[[08/28/2017 03:38:43 PM]] [[step      181]]     [[train]]     loss: 186.597229       [[val]]     loss: 173.53398132\n",
      "[[08/28/2017 03:38:45 PM]] [[step      182]]     [[train]]     loss: 83.75813293      [[val]]     loss: 166.22071838\n",
      "[[08/28/2017 03:38:47 PM]] [[step      183]]     [[train]]     loss: 114.96929932     [[val]]     loss: 171.3968811 \n",
      "[[08/28/2017 03:38:49 PM]] [[step      184]]     [[train]]     loss: 180.99531555     [[val]]     loss: 170.99172974\n",
      "[[08/28/2017 03:38:51 PM]] [[step      185]]     [[train]]     loss: 119.9521637      [[val]]     loss: 50.73941803 \n",
      "[[08/28/2017 03:38:52 PM]] [[step      186]]     [[train]]     loss: 110.09757233     [[val]]     loss: 140.92565918\n",
      "[[08/28/2017 03:38:54 PM]] [[step      187]]     [[train]]     loss: 103.39707947     [[val]]     loss: 130.55384827\n",
      "[[08/28/2017 03:38:56 PM]] [[step      188]]     [[train]]     loss: 147.98878479     [[val]]     loss: 146.54838562\n",
      "[[08/28/2017 03:38:58 PM]] [[step      189]]     [[train]]     loss: 186.67446899     [[val]]     loss: 188.50010681\n",
      "[[08/28/2017 03:39:00 PM]] [[step      190]]     [[train]]     loss: 101.08144379     [[val]]     loss: 153.94197083\n",
      "[[08/28/2017 03:39:01 PM]] [[step      191]]     [[train]]     loss: 52.87688446      [[val]]     loss: 153.92054749\n",
      "[[08/28/2017 03:39:03 PM]] [[step      192]]     [[train]]     loss: 49.86147308      [[val]]     loss: 161.9460144 \n",
      "[[08/28/2017 03:39:05 PM]] [[step      193]]     [[train]]     loss: 37.95425415      [[val]]     loss: 184.77352905\n",
      "[[08/28/2017 03:39:07 PM]] [[step      194]]     [[train]]     loss: 142.49542236     [[val]]     loss: 145.18972778\n",
      "[[08/28/2017 03:39:08 PM]] [[step      195]]     [[train]]     loss: 68.85166168      [[val]]     loss: 124.31500244\n",
      "[[08/28/2017 03:39:10 PM]] [[step      196]]     [[train]]     loss: 33.9118309       [[val]]     loss: 165.91069031\n",
      "[[08/28/2017 03:39:12 PM]] [[step      197]]     [[train]]     loss: 56.1059761       [[val]]     loss: 161.40693665\n",
      "[[08/28/2017 03:39:14 PM]] [[step      198]]     [[train]]     loss: 142.46960449     [[val]]     loss: 57.03386688 \n",
      "[[08/28/2017 03:39:15 PM]] [[step      199]]     [[train]]     loss: 117.62635803     [[val]]     loss: 111.60958099\n",
      "[[08/28/2017 03:39:17 PM]] [[step      200]]     [[train]]     loss: 153.37808228     [[val]]     loss: 183.82539368\n",
      "[[08/28/2017 03:39:19 PM]] [[step      201]]     [[train]]     loss: 122.865448       [[val]]     loss: 170.54130554\n",
      "[[08/28/2017 03:39:21 PM]] [[step      202]]     [[train]]     loss: 151.24673462     [[val]]     loss: 154.7809906 \n",
      "[[08/28/2017 03:39:23 PM]] [[step      203]]     [[train]]     loss: 102.83718109     [[val]]     loss: 191.18151855\n",
      "[[08/28/2017 03:39:24 PM]] [[step      204]]     [[train]]     loss: 97.47582245      [[val]]     loss: 75.18644714 \n",
      "[[08/28/2017 03:39:26 PM]] [[step      205]]     [[train]]     loss: 200.0            [[val]]     loss: 172.04695129\n",
      "[[08/28/2017 03:39:28 PM]] [[step      206]]     [[train]]     loss: 67.80492401      [[val]]     loss: 49.34874725 \n",
      "[[08/28/2017 03:39:30 PM]] [[step      207]]     [[train]]     loss: 57.3059845       [[val]]     loss: 120.80113983\n",
      "[[08/28/2017 03:39:31 PM]] [[step      208]]     [[train]]     loss: 101.49512482     [[val]]     loss: 174.66465759\n",
      "[[08/28/2017 03:39:33 PM]] [[step      209]]     [[train]]     loss: 119.44378662     [[val]]     loss: 39.89739609 \n",
      "[[08/28/2017 03:39:35 PM]] [[step      210]]     [[train]]     loss: 127.1195755      [[val]]     loss: 139.00448608\n",
      "[[08/28/2017 03:39:37 PM]] [[step      211]]     [[train]]     loss: 107.2530365      [[val]]     loss: 69.41165161 \n",
      "[[08/28/2017 03:39:39 PM]] [[step      212]]     [[train]]     loss: 80.29377747      [[val]]     loss: 140.82868958\n",
      "[[08/28/2017 03:39:40 PM]] [[step      213]]     [[train]]     loss: 194.82904053     [[val]]     loss: 34.70877838 \n",
      "[[08/28/2017 03:39:42 PM]] [[step      214]]     [[train]]     loss: 185.95184326     [[val]]     loss: 137.92926025\n",
      "[[08/28/2017 03:39:44 PM]] [[step      215]]     [[train]]     loss: 141.68579102     [[val]]     loss: 177.19696045\n",
      "[[08/28/2017 03:39:46 PM]] [[step      216]]     [[train]]     loss: 174.11975098     [[val]]     loss: 200.0       \n",
      "[[08/28/2017 03:39:47 PM]] [[step      217]]     [[train]]     loss: 148.28684998     [[val]]     loss: 194.83598328\n",
      "[[08/28/2017 03:39:49 PM]] [[step      218]]     [[train]]     loss: 177.83526611     [[val]]     loss: 48.88794708 \n",
      "[[08/28/2017 03:39:51 PM]] [[step      219]]     [[train]]     loss: 64.18067932      [[val]]     loss: 166.08964539\n",
      "[[08/28/2017 03:39:53 PM]] [[step      220]]     [[train]]     loss: 144.29966736     [[val]]     loss: 124.58966827\n",
      "[[08/28/2017 03:39:55 PM]] [[step      221]]     [[train]]     loss: 165.07905579     [[val]]     loss: 171.97360229\n",
      "[[08/28/2017 03:39:56 PM]] [[step      222]]     [[train]]     loss: 174.57958984     [[val]]     loss: 192.36470032\n",
      "[[08/28/2017 03:39:58 PM]] [[step      223]]     [[train]]     loss: 140.99118042     [[val]]     loss: 50.23271561 \n",
      "[[08/28/2017 03:40:00 PM]] [[step      224]]     [[train]]     loss: 181.2270813      [[val]]     loss: 87.18712616 \n",
      "[[08/28/2017 03:40:02 PM]] [[step      225]]     [[train]]     loss: 112.40696716     [[val]]     loss: 75.83383942 \n",
      "[[08/28/2017 03:40:03 PM]] [[step      226]]     [[train]]     loss: 60.96794128      [[val]]     loss: 141.08306885\n",
      "[[08/28/2017 03:40:05 PM]] [[step      227]]     [[train]]     loss: 103.28220367     [[val]]     loss: 150.09701538\n",
      "[[08/28/2017 03:40:07 PM]] [[step      228]]     [[train]]     loss: 70.72924042      [[val]]     loss: 143.2539978 \n",
      "[[08/28/2017 03:40:09 PM]] [[step      229]]     [[train]]     loss: 167.6254425      [[val]]     loss: 44.24455261 \n",
      "[[08/28/2017 03:40:11 PM]] [[step      230]]     [[train]]     loss: 179.71815491     [[val]]     loss: 200.0       \n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    # 100 steps make an epoch\n",
    "    nn = WikiRNN(\n",
    "        name='v0',\n",
    "        reader=reader,\n",
    "        work_dir='./tf-data',\n",
    "        optimizer='adam',\n",
    "        learning_rate=1.0001,\n",
    "        batch_size=1,\n",
    "        num_validation_batches=1,\n",
    "        num_training_steps=10000,\n",
    "        early_stopping_steps=300,\n",
    "        num_restarts=3,\n",
    "        warm_start_init_step=0,\n",
    "        regularization_constant=0.0,\n",
    "        enable_parameter_averaging=False,\n",
    "        min_steps_to_checkpoint=0,\n",
    "        loss_averaging_window=1,\n",
    "        log_interval=1,\n",
    "        \n",
    "        state_size=10, \n",
    "        keep_prob=1\n",
    "    )\n",
    "    nn.fit()\n",
    "#     nn.restore()\n",
    "#     preds = nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time\n",
    "from subprocess import check_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_df.csv\n",
      "key_1.csv\n",
      "median_49.csv\n",
      "page_df.csv\n",
      "page_ohe.csv\n",
      "sample_submission_1.csv\n",
      "spider.txt\n",
      "train_1.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_paths = [\n",
    "    \"/data/kaggle-wikipedia/data/\",\n",
    "    \"/Users/jiayou/Dropbox/JuanCode/Kaggle/Wikipedia/data/\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/JuanCode/Kaggle/Wikipedia/data/\"\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "print(check_output([\"ls\", root]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(root + 'train_1.csv')\n",
    "train.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = train.iloc[:,1:].values\n",
    "scale = np.percentile(data, 98, axis=1)\n",
    "data = data / np.maximum(1, scale.reshape((data.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpage = pd.read_csv(os.path.join(root, 'page_df.csv'))\n",
    "fdate = pd.read_csv(os.path.join(root, 'date_df.csv'))\n",
    "\n",
    "fdate.isweekday = fdate.isweekday.astype(np.int32)\n",
    "fdate.drop(['date', 'date_str'], axis=1, inplace=True)\n",
    "\n",
    "fpage_depth = {}\n",
    "for c in ['domain', 'access', 'agent']:\n",
    "    values = fpage[c].unique()\n",
    "    n = len(values)\n",
    "    fpage_depth[c] = n\n",
    "    idx_lookup = {values[i]: i for i in range(n)}\n",
    "    def v2idx(v):\n",
    "        return idx_lookup[v]\n",
    "    fpage[c] = np.vectorize(v2idx)(fpage[c])\n",
    "fpage.drop(['Page'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os, imp\n",
    "import pprint as pp\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]\n",
    "\n",
    "class TFBaseModel(object):\n",
    "\n",
    "    \"\"\"Interface containing some boilerplate code for training tensorflow models.\n",
    "    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n",
    "    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n",
    "    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n",
    "    and ending with the loss tensor.\n",
    "    Args:\n",
    "        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n",
    "            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n",
    "        batch_size: Minibatch size.\n",
    "        learning_rate: Learning rate.\n",
    "        optimizer: 'rms' for RMSProp, 'adam' for Adam, 'sgd' for SGD\n",
    "        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "        regularization_constant:  Regularization constant applied to all trainable parameters.\n",
    "        early_stopping_steps:  Number of steps to continue training after validation loss has\n",
    "            stopped decreasing.\n",
    "        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n",
    "            at warm_start_init_step.\n",
    "        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n",
    "            learning rate will be halved.  This process will repeat num_restarts times.\n",
    "        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n",
    "            to separate checkpoint file.\n",
    "        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n",
    "            have passed.\n",
    "        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n",
    "        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n",
    "            training steps.\n",
    "        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n",
    "        log_dir: Directory where logs are written.\n",
    "        checkpoint_dir: Directory where checkpoints are saved.\n",
    "        prediction_dir: Directory where predictions/outputs are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reader,\n",
    "        batch_size=128,\n",
    "        num_training_steps=20000,\n",
    "        learning_rate=.01,\n",
    "        optimizer='adam',\n",
    "        grad_clip=5,\n",
    "        regularization_constant=0.0,\n",
    "        early_stopping_steps=3000,\n",
    "        warm_start_init_step=0,\n",
    "        num_restarts=None,\n",
    "        enable_parameter_averaging=False,\n",
    "        min_steps_to_checkpoint=100,\n",
    "        log_interval=20,\n",
    "        loss_averaging_window=100,\n",
    "        num_validation_batches=1,\n",
    "        work_dir='tf-data',\n",
    "        name='nn'\n",
    "    ):\n",
    "\n",
    "        self.reader = reader\n",
    "        self.batch_size = batch_size\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_clip = grad_clip\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.warm_start_init_step = warm_start_init_step\n",
    "        self.early_stopping_steps = early_stopping_steps if early_stopping_steps is not None else np.inf\n",
    "        self.enable_parameter_averaging = enable_parameter_averaging\n",
    "        self.num_restarts = num_restarts\n",
    "        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n",
    "        self.log_interval = log_interval\n",
    "        self.num_validation_batches = num_validation_batches\n",
    "        self.loss_averaging_window = loss_averaging_window\n",
    "        self.name = name\n",
    "\n",
    "        self.log_dir = os.path.join(work_dir, 'logs')\n",
    "        self.prediction_dir = os.path.join(work_dir, 'predictions')\n",
    "        self.checkpoint_dir = os.path.join(work_dir, 'checkpoints')\n",
    "        if self.enable_parameter_averaging:\n",
    "            self.checkpoint_dir_averaged = os.path.join(work_dir, 'checkpoints-avg')\n",
    "\n",
    "        self.init_logging(self.log_dir)\n",
    "        self.logger.info('\\nNetwork hyper-parameters:\\n{}'.format(pp.pformat(self.__dict__)))\n",
    "        self.reader.describe(self.logger)\n",
    "\n",
    "        self.graph = self.build_graph()\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        print('Built graph')\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        raise NotImplementedError('subclass must implement this')\n",
    "\n",
    "    def fit(self, yield_interval=None):\n",
    "        with self.session.as_default():\n",
    "\n",
    "            if self.warm_start_init_step:\n",
    "                self.restore(self.warm_start_init_step)\n",
    "                step = self.warm_start_init_step\n",
    "            else:\n",
    "                self.session.run(self.init)\n",
    "                step = 0\n",
    "\n",
    "            train_generator = self.reader.train_batch_generator(self.batch_size)\n",
    "            val_generator = self.reader.val_batch_generator(self.num_validation_batches*self.batch_size)\n",
    "\n",
    "            train_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "            val_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "\n",
    "            best_validation_loss, best_validation_tstep = float('inf'), 0\n",
    "            restarts = 0\n",
    "\n",
    "            while step < self.num_training_steps:\n",
    "                \n",
    "                if yield_interval is not None and step % yield_interval == 0:\n",
    "                    yield step\n",
    "\n",
    "                # validation evaluation\n",
    "                val_batch_df = next(val_generator)\n",
    "                val_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in val_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                val_feed_dict.update({self.learning_rate_var: self.learning_rate})\n",
    "                [val_loss] = self.session.run(\n",
    "                    fetches=[self.loss],\n",
    "                    feed_dict=val_feed_dict\n",
    "                )\n",
    "                val_loss_history.append(val_loss)\n",
    "\n",
    "                # train step\n",
    "                train_batch_df = next(train_generator)\n",
    "                train_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in train_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                train_feed_dict.update({self.learning_rate_var: self.learning_rate})\n",
    "                train_loss, _ = self.session.run(\n",
    "                    fetches=[self.loss, self.step],\n",
    "                    feed_dict=train_feed_dict\n",
    "                )\n",
    "                train_loss_history.append(train_loss)\n",
    "\n",
    "                if step % self.log_interval == 0 or step + 1 == self.num_training_steps:\n",
    "                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
    "                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
    "                    metric_log = (\n",
    "                        \"[[step {:>8}]]     \"\n",
    "                        \"[[train]]     loss: {:<12}     \"\n",
    "                        \"[[val]]     loss: {:<12}\"\n",
    "                    ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n",
    "                    self.logger.info(metric_log)\n",
    "\n",
    "                    if step > self.min_steps_to_checkpoint and avg_val_loss < best_validation_loss:\n",
    "                        best_validation_loss = avg_val_loss\n",
    "                        best_validation_tstep = step\n",
    "                        self.save(step)\n",
    "                        if self.enable_parameter_averaging:\n",
    "                            self.save(step, averaged=True)\n",
    "\n",
    "                    if step - best_validation_tstep >= self.early_stopping_steps:\n",
    "\n",
    "                        if self.num_restarts is None or restarts >= self.num_restarts:\n",
    "                            self.logger.info('Early stopping')\n",
    "                            break\n",
    "\n",
    "                        if restarts < self.num_restarts:\n",
    "                            self.logger.info('')\n",
    "                            self.restore(best_validation_tstep)\n",
    "                            self.learning_rate /= 2.0\n",
    "#                             self.early_stopping_steps /= 2\n",
    "                            step = best_validation_tstep\n",
    "                            restarts += 1\n",
    "                            self.logger.info(\n",
    "                                'Half learning rate to {} and restore step {}'.format(self.learning_rate, step)\n",
    "                            )\n",
    "\n",
    "                step += 1\n",
    "\n",
    "            if step <= self.min_steps_to_checkpoint:\n",
    "                best_validation_tstep = step\n",
    "                self.save(step)\n",
    "                if self.enable_parameter_averaging:\n",
    "                    self.save(step, averaged=True)\n",
    "\n",
    "            self.logger.info('Training ended')\n",
    "            self.logger.info(\n",
    "                'Best validation loss of {} at training step {}'.format(\n",
    "                    round(best_validation_loss, 8), \n",
    "                    best_validation_tstep\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def predict(self, batch_size=128, num_batches=None):\n",
    "        if not os.path.isdir(self.prediction_dir):\n",
    "            os.makedirs(self.prediction_dir)\n",
    "\n",
    "        preds = {}\n",
    "        if hasattr(self, 'prediction_tensors'):\n",
    "            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n",
    "\n",
    "            test_generator = self.reader.test_batch_generator(batch_size)\n",
    "            for i, test_batch_df in enumerate(test_generator):\n",
    "                test_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n",
    "                np_tensors = self.session.run(\n",
    "                    fetches=tf_tensors,\n",
    "                    feed_dict=test_feed_dict\n",
    "                )\n",
    "                for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "                    prediction_dict[tensor_name].append(tensor)\n",
    "                    \n",
    "                if num_batches is not None and i + 1 == num_batches:\n",
    "                    break\n",
    "\n",
    "            for tensor_name, tensor in prediction_dict.items():\n",
    "                np_tensor = np.concatenate(tensor, 0)\n",
    "                preds[tensor_name] = np_tensor\n",
    "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "                self.logger.info('Saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "                np.save(save_file, np_tensor)\n",
    "\n",
    "        if hasattr(self, 'parameter_tensors'):\n",
    "            for tensor_name, tensor in self.parameter_tensors.items():\n",
    "                np_tensor = tensor.eval(self.session)\n",
    "\n",
    "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "                self.logger.info('Saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "                np.save(save_file, np_tensor)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def save(self, step, averaged=False):\n",
    "        saver = self.saver_averaged if averaged else self.saver\n",
    "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
    "        if not os.path.isdir(checkpoint_dir):\n",
    "            self.logger.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
    "            os.mkdir(checkpoint_dir)\n",
    "\n",
    "        model_path = os.path.join(checkpoint_dir, 'model')\n",
    "        self.logger.info('saving model to {}'.format(model_path))\n",
    "        saver.save(self.session, model_path, global_step=step)\n",
    "\n",
    "    def restore(self, step=None, averaged=False):\n",
    "        saver = self.saver_averaged if averaged else self.saver\n",
    "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
    "        if not step:\n",
    "            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "            self.logger.info('Restoring model parameters from {}'.format(model_path))\n",
    "            saver.restore(self.session, model_path)\n",
    "        else:\n",
    "            model_path = os.path.join(\n",
    "                checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
    "            )\n",
    "            self.logger.info('Restoring model from {}'.format(model_path))\n",
    "            saver.restore(self.session, model_path)\n",
    "\n",
    "    def init_logging(self, log_dir):\n",
    "        if not os.path.isdir(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        logger = logging.getLogger(\n",
    "            '{}.{}'.format(\n",
    "                type(self).__name__, \n",
    "                datetime.now().strftime('%Y-%m-%d.%H-%M-%S.%f')\n",
    "            )\n",
    "        )\n",
    "        logger.setLevel(logging.INFO)\n",
    "        fmtr = logging.Formatter(\n",
    "            fmt='[[%(asctime)s]] %(message)s',\n",
    "            datefmt='%m/%d/%Y %I:%M:%S %p'\n",
    "        )\n",
    "\n",
    "        h = logging.StreamHandler(stream=sys.stdout)\n",
    "        h.setFormatter(fmtr)\n",
    "        logger.addHandler(h)\n",
    "        \n",
    "        date_str = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        log_file = 'log.{}.{}.{}.txt'.format(type(self).__name__, self.name, date_str)\n",
    "        h = logging.FileHandler(filename=os.path.join(log_dir, log_file))\n",
    "        h.setFormatter(fmtr)\n",
    "        logger.addHandler(h)\n",
    "        \n",
    "        self.logger = logger\n",
    "\n",
    "    def update_parameters(self, loss):\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.learning_rate_var = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        if self.regularization_constant != 0:\n",
    "            l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n",
    "            loss = loss + self.regularization_constant*l2_norm\n",
    "\n",
    "        optimizer = self.get_optimizer(self.learning_rate_var)\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        clipped = [\n",
    "            (g if g is None else tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads\n",
    "        ]\n",
    "\n",
    "        step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n",
    "\n",
    "        if self.enable_parameter_averaging:\n",
    "            maintain_averages_op = self.ema.apply(tf.trainable_variables())\n",
    "            with tf.control_dependencies([step]):\n",
    "                self.step = tf.group(maintain_averages_op)\n",
    "        else:\n",
    "            self.step = step\n",
    "\n",
    "    def log_parameters(self):\n",
    "        self.logger.info(\n",
    "            (\n",
    "                '\\n\\n'\n",
    "                'All parameters:\\n{}\\n'\n",
    "                'Trainable parameters:\\n{}\\n'\n",
    "                'Trainable parameter count: {}\\n'\n",
    "            ).format(\n",
    "                pp.pformat([(var.name, shape(var)) for var in tf.global_variables()]),\n",
    "                pp.pformat([(var.name, shape(var)) for var in tf.trainable_variables()]),\n",
    "                str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_optimizer(self, learning_rate):\n",
    "        if self.optimizer == 'adam':\n",
    "            return tf.train.AdamOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'gd':\n",
    "            return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'rms':\n",
    "            return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "        else:\n",
    "            assert False, 'optimizer must be adam, gd, or rms'\n",
    "\n",
    "    def build_graph(self):\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            self.ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "\n",
    "            self.loss = self.calculate_loss()\n",
    "            self.update_parameters(self.loss)\n",
    "            self.log_parameters()\n",
    "\n",
    "            self.saver = tf.train.Saver(max_to_keep=1)\n",
    "            if self.enable_parameter_averaging:\n",
    "                self.saver_averaged = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "            return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self, data, fpage, fdate, seed=923):\n",
    "        self.data = data\n",
    "        self.seed = seed\n",
    "        self.days = self.data.shape[1]\n",
    "        self.train_days = 400\n",
    "        \n",
    "        self.domain = fpage.domain.values\n",
    "        self.access = fpage.access.values\n",
    "        self.agent = fpage.agent.values\n",
    "        \n",
    "        self.dayofweek = fdate.dayofweek.values\n",
    "        self.isweekday = fdate.isweekday.values\n",
    "        self.month = fdate.month.values\n",
    "        \n",
    "    def describe(self, logger):\n",
    "        logger.info('')\n",
    "        logger.info('Data dimensions:')\n",
    "        logger.info('    [[data]] {}'.format(self.data.shape))\n",
    "        logger.info('Split seed = {}'.format(self.seed))\n",
    "        logger.info('')\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            self.data,\n",
    "            batch_size=batch_size,\n",
    "            start_day = 0,\n",
    "            total_days = self.train_days,\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            self.data,\n",
    "            batch_size=batch_size,\n",
    "            start_day = self.train_days,\n",
    "            total_days = self.days - self.train_days,\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        start = 0\n",
    "        n = self.data.shape[0]\n",
    "        while start < n:\n",
    "            batch = {}\n",
    "            idx = [i for i in range(start, min(start + batch_size, n))]\n",
    "            batch['data'] = self.data[idx, :]\n",
    "            batch['given_days'] = self.days - 62\n",
    "            batch['no_loss_days'] = 0\n",
    "            batch['days'] = self.days\n",
    "            \n",
    "            batch['dayofweek'] = self.dayofweek\n",
    "            batch['isweekday'] = self.isweekday\n",
    "            batch['month'] = self.month\n",
    "            \n",
    "            batch['domain'] = self.domain[idx]\n",
    "            batch['agent'] = self.agent[idx]\n",
    "            batch['access'] = self.access[idx]\n",
    "            \n",
    "            yield batch\n",
    "            \n",
    "            start += batch_size\n",
    "\n",
    "    def batch_generator(self, data, batch_size, start_day, total_days):\n",
    "        while True:\n",
    "            idx = np.random.randint(0, data.shape[0], [batch_size])\n",
    "            start = np.random.randint(start_day, start_day + total_days - 60)\n",
    "            days = np.random.randint(60, start_day + total_days - start)\n",
    "            days_idx = [i for i in range(start, start + days)]\n",
    "            given_days = days - 1\n",
    "            no_loss_days = days - 1\n",
    "            \n",
    "            batch = {}\n",
    "            batch['data'] = data[idx, :][:, days_idx]\n",
    "            \n",
    "            batch['given_days'] = given_days\n",
    "            batch['no_loss_days'] = no_loss_days\n",
    "            batch['days'] = days\n",
    "            \n",
    "            batch['dayofweek'] = self.dayofweek[days_idx]\n",
    "            batch['isweekday'] = self.isweekday[days_idx]\n",
    "            batch['month'] = self.month[days_idx]\n",
    "            \n",
    "            batch['domain'] = self.domain[idx]\n",
    "            batch['agent'] = self.agent[idx]\n",
    "            batch['access'] = self.access[idx]\n",
    "            \n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiRNN(TFBaseModel):\n",
    "\n",
    "    def __init__(self, state_size, keep_prob=1, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        super(type(self), self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        self.data = tf.placeholder(tf.float32, name='data')\n",
    "        self.given_days = tf.placeholder(tf.int32, name='given_days')\n",
    "        self.no_loss_days = tf.placeholder(tf.int32, name='no_loss_days')\n",
    "        self.days = tf.placeholder(tf.int32, name='days')\n",
    "        batch_size = tf.shape(self.data)[0]\n",
    "        \n",
    "        # Features\n",
    "        self.dayofweek = tf.placeholder(tf.int32, [None])\n",
    "        self.isweekday = tf.placeholder(tf.int32, [None])\n",
    "        self.month = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self.domain = tf.placeholder(tf.int32, [None])\n",
    "        self.agent = tf.placeholder(tf.int32, [None])\n",
    "        self.access = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        dayofweek_oh = tf.one_hot(self.dayofweek, 7)\n",
    "        isweekday_oh = tf.one_hot(self.isweekday, 2)\n",
    "        month = tf.one_hot(self.month, 13)\n",
    "        \n",
    "        domain = tf.one_hot(self.domain, 9)\n",
    "        agent = tf.one_hot(self.agent, 2)\n",
    "        access = tf.one_hot(self.access, 3)\n",
    "        \n",
    "        date_features = tf.concat(\n",
    "            [\n",
    "                dayofweek_oh,\n",
    "                isweekday_oh,\n",
    "                month,\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        date_features = tf.tile(tf.expand_dims(date_features, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        page_features = tf.concat(\n",
    "            [\n",
    "                domain,\n",
    "                agent,\n",
    "                access,\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        page_features = tf.tile(tf.expand_dims(page_features, 1), [1, self.days, 1])\n",
    "        \n",
    "        features = tf.concat([date_features, page_features], axis=2)\n",
    "        \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(\n",
    "                self.state_size\n",
    "            ),\n",
    "            output_keep_prob=self.keep_prob\n",
    "        )\n",
    "        \n",
    "        # [batch_size, state_size]\n",
    "        state = cell.zero_state(tf.shape(self.data)[0], dtype=tf.float32)\n",
    "        # [batch_size, 1]\n",
    "        last_output = tf.zeros([tf.shape(self.data)[0], 1], dtype=tf.float32)\n",
    "        \n",
    "        loss = tf.constant(0, dtype=tf.float32)\n",
    "        step = tf.constant(0, dtype=tf.int32)\n",
    "        output_ta = tf.TensorArray(size=self.days, dtype=tf.float32)\n",
    "        \n",
    "        def cond(last_output, state, loss, step, output_ta):\n",
    "            return step < self.days\n",
    "        \n",
    "        def body(last_output, state, loss, step, output_ta):\n",
    "            inp = tf.concat(\n",
    "                [\n",
    "                    last_output,\n",
    "                    features[:, step, :],\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            output, state = cell(inp, state)\n",
    "            output = tf.layers.dense(\n",
    "                output,\n",
    "                1,\n",
    "                activation=tf.nn.relu,\n",
    "                name='dense-top'\n",
    "            )\n",
    "            output_ta = output_ta.write(step, tf.transpose(output))\n",
    "            \n",
    "            last_output = tf.cond(\n",
    "                step < self.given_days,\n",
    "                lambda: tf.expand_dims(self.data[:,step], 1),\n",
    "                lambda: output,\n",
    "            )\n",
    "            last_output.set_shape([None, 1])\n",
    "            \n",
    "            true = tf.maximum(1e-8, self.data[:,step])\n",
    "            loss = tf.cond(\n",
    "                step >= self.no_loss_days,\n",
    "                lambda: loss + tf.reduce_mean(2 * tf.abs(true - output) / tf.maximum(1e-6, true + output)),\n",
    "                lambda: loss\n",
    "            )\n",
    "            loss.set_shape([])\n",
    "            \n",
    "            return (last_output, state, loss, step + 1, output_ta)\n",
    "        \n",
    "        _, _, loss, _, output_ta = tf.while_loop(\n",
    "            cond=cond,\n",
    "            body=body,\n",
    "            loop_vars=(last_output, state, loss, step, output_ta)\n",
    "        )\n",
    "        \n",
    "        self.preds = tf.transpose(output_ta.concat())\n",
    "        self.prediction_tensors = {\n",
    "            'preds': self.preds\n",
    "        }\n",
    "        \n",
    "        loss = loss / tf.cast(self.days - self.no_loss_days, tf.float32)\n",
    "        loss = tf.Print(loss, [loss, self.data[:, -1], self.preds[:, -1]], \"Loss = \")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = DataReader(data, fpage=fpage, fdate=fdate, seed=923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[08/31/2017 04:47:28 PM]] \n",
      "Network hyper-parameters:\n",
      "{'batch_size': 1,\n",
      " 'checkpoint_dir': './tf-data/checkpoints',\n",
      " 'early_stopping_steps': 300,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'grad_clip': 5,\n",
      " 'keep_prob': 1,\n",
      " 'learning_rate': 0.0001,\n",
      " 'log_dir': './tf-data/logs',\n",
      " 'log_interval': 1,\n",
      " 'logger': <Logger WikiRNN.2017-08-31.16-47-28.684086 (INFO)>,\n",
      " 'loss_averaging_window': 100,\n",
      " 'min_steps_to_checkpoint': 50,\n",
      " 'name': 'v1',\n",
      " 'num_restarts': 3,\n",
      " 'num_training_steps': 100,\n",
      " 'num_validation_batches': 1,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': './tf-data/predictions',\n",
      " 'reader': <__main__.DataReader object at 0x12a88b7f0>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'state_size': 10,\n",
      " 'warm_start_init_step': 0}\n",
      "[[08/31/2017 04:47:28 PM]] \n",
      "[[08/31/2017 04:47:28 PM]] Data dimensions:\n",
      "[[08/31/2017 04:47:28 PM]]     [[data]] (145063, 550)\n",
      "[[08/31/2017 04:47:28 PM]] Split seed = 923\n",
      "[[08/31/2017 04:47:28 PM]] \n",
      "[[08/31/2017 04:47:30 PM]] \n",
      "\n",
      "All parameters:\n",
      "[('lstm_cell/weights:0', [47, 40]),\n",
      " ('lstm_cell/biases:0', [40]),\n",
      " ('dense-top/kernel:0', [10, 1]),\n",
      " ('dense-top/bias:0', [1]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('lstm_cell/weights/Adam:0', [47, 40]),\n",
      " ('lstm_cell/weights/Adam_1:0', [47, 40]),\n",
      " ('lstm_cell/biases/Adam:0', [40]),\n",
      " ('lstm_cell/biases/Adam_1:0', [40]),\n",
      " ('dense-top/kernel/Adam:0', [10, 1]),\n",
      " ('dense-top/kernel/Adam_1:0', [10, 1]),\n",
      " ('dense-top/bias/Adam:0', [1]),\n",
      " ('dense-top/bias/Adam_1:0', [1])]\n",
      "Trainable parameters:\n",
      "[('lstm_cell/weights:0', [47, 40]),\n",
      " ('lstm_cell/biases:0', [40]),\n",
      " ('dense-top/kernel:0', [10, 1]),\n",
      " ('dense-top/bias:0', [1])]\n",
      "Trainable parameter count: 1931\n",
      "\n",
      "Built graph\n",
      "[[08/31/2017 04:47:32 PM]] [[step        0]]     [[train]]     loss: 2.0              [[val]]     loss: 2.0         \n",
      "[[08/31/2017 04:47:33 PM]] [[step        1]]     [[train]]     loss: 2.0              [[val]]     loss: 1.24918549  \n",
      "[[08/31/2017 04:47:34 PM]] [[step        2]]     [[train]]     loss: 1.44614134       [[val]]     loss: 1.49945699  \n",
      "[[08/31/2017 04:47:34 PM]] [[step        3]]     [[train]]     loss: 1.58460601       [[val]]     loss: 1.62459274  \n",
      "[[08/31/2017 04:47:36 PM]] [[step        4]]     [[train]]     loss: 1.58983415       [[val]]     loss: 1.49628531  \n",
      "[[08/31/2017 04:47:37 PM]] [[step        5]]     [[train]]     loss: 1.65819513       [[val]]     loss: 1.47855534  \n",
      "[[08/31/2017 04:47:38 PM]] [[step        6]]     [[train]]     loss: 1.7070244        [[val]]     loss: 1.50390749  \n",
      "[[08/31/2017 04:47:40 PM]] [[step        7]]     [[train]]     loss: 1.74364635       [[val]]     loss: 1.56591905  \n",
      "[[08/31/2017 04:47:41 PM]] [[step        8]]     [[train]]     loss: 1.77213009       [[val]]     loss: 1.61415027  \n",
      "[[08/31/2017 04:47:42 PM]] [[step        9]]     [[train]]     loss: 1.79491708       [[val]]     loss: 1.65273524  \n",
      "[[08/31/2017 04:47:42 PM]] [[step       10]]     [[train]]     loss: 1.63356098       [[val]]     loss: 1.50828855  \n",
      "[[08/31/2017 04:47:44 PM]] [[step       11]]     [[train]]     loss: 1.63576927       [[val]]     loss: 1.54926451  \n",
      "[[08/31/2017 04:47:45 PM]] [[step       12]]     [[train]]     loss: 1.66378702       [[val]]     loss: 1.44632669  \n",
      "[[08/31/2017 04:47:45 PM]] [[step       13]]     [[train]]     loss: 1.60191408       [[val]]     loss: 1.48587479  \n",
      "[[08/31/2017 04:47:46 PM]] [[step       14]]     [[train]]     loss: 1.61226253       [[val]]     loss: 1.5201498   \n",
      "[[08/31/2017 04:47:47 PM]] [[step       15]]     [[train]]     loss: 1.63649612       [[val]]     loss: 1.45728477  \n",
      "[[08/31/2017 04:47:48 PM]] [[step       16]]     [[train]]     loss: 1.6578787        [[val]]     loss: 1.46349637  \n",
      "[[08/31/2017 04:47:49 PM]] [[step       17]]     [[train]]     loss: 1.67688544       [[val]]     loss: 1.46251837  \n",
      "[[08/31/2017 04:47:51 PM]] [[step       18]]     [[train]]     loss: 1.69389147       [[val]]     loss: 1.4583055   \n",
      "[[08/31/2017 04:47:52 PM]] [[step       19]]     [[train]]     loss: 1.7091969        [[val]]     loss: 1.48539023  \n",
      "[[08/31/2017 04:47:53 PM]] [[step       20]]     [[train]]     loss: 1.72304466       [[val]]     loss: 1.50989545  \n",
      "[[08/31/2017 04:47:53 PM]] [[step       21]]     [[train]]     loss: 1.73563354       [[val]]     loss: 1.53217293  \n",
      "[[08/31/2017 04:47:54 PM]] [[step       22]]     [[train]]     loss: 1.74712774       [[val]]     loss: 1.55251324  \n",
      "[[08/31/2017 04:47:54 PM]] [[step       23]]     [[train]]     loss: 1.7135401        [[val]]     loss: 1.57115852  \n",
      "[[08/31/2017 04:47:55 PM]] [[step       24]]     [[train]]     loss: 1.66151494       [[val]]     loss: 1.58831218  \n",
      "[[08/31/2017 04:47:56 PM]] [[step       25]]     [[train]]     loss: 1.67453359       [[val]]     loss: 1.58731653  \n",
      "[[08/31/2017 04:47:57 PM]] [[step       26]]     [[train]]     loss: 1.6865879        [[val]]     loss: 1.6026011   \n",
      "[[08/31/2017 04:47:58 PM]] [[step       27]]     [[train]]     loss: 1.6270669        [[val]]     loss: 1.61679392  \n",
      "[[08/31/2017 04:47:59 PM]] [[step       28]]     [[train]]     loss: 1.59639161       [[val]]     loss: 1.63000792  \n",
      "[[08/31/2017 04:48:00 PM]] [[step       29]]     [[train]]     loss: 1.56538025       [[val]]     loss: 1.57634099  \n",
      "[[08/31/2017 04:48:01 PM]] [[step       30]]     [[train]]     loss: 1.57940024       [[val]]     loss: 1.59000741  \n",
      "[[08/31/2017 04:48:02 PM]] [[step       31]]     [[train]]     loss: 1.54804469       [[val]]     loss: 1.60281968  \n",
      "[[08/31/2017 04:48:02 PM]] [[step       32]]     [[train]]     loss: 1.5017403        [[val]]     loss: 1.60754797  \n",
      "[[08/31/2017 04:48:03 PM]] [[step       33]]     [[train]]     loss: 1.516395         [[val]]     loss: 1.60526939  \n",
      "[[08/31/2017 04:48:04 PM]] [[step       34]]     [[train]]     loss: 1.47364086       [[val]]     loss: 1.61654741  \n",
      "[[08/31/2017 04:48:05 PM]] [[step       35]]     [[train]]     loss: 1.48341676       [[val]]     loss: 1.58587417  \n",
      "[[08/31/2017 04:48:06 PM]] [[step       36]]     [[train]]     loss: 1.45098655       [[val]]     loss: 1.59706676  \n",
      "[[08/31/2017 04:48:07 PM]] [[step       37]]     [[train]]     loss: 1.46543427       [[val]]     loss: 1.60767026  \n",
      "[[08/31/2017 04:48:08 PM]] [[step       38]]     [[train]]     loss: 1.47914108       [[val]]     loss: 1.58031135  \n",
      "[[08/31/2017 04:48:09 PM]] [[step       39]]     [[train]]     loss: 1.49216256       [[val]]     loss: 1.59080357  \n",
      "[[08/31/2017 04:48:09 PM]] [[step       40]]     [[train]]     loss: 1.50454884       [[val]]     loss: 1.60078397  \n",
      "[[08/31/2017 04:48:10 PM]] [[step       41]]     [[train]]     loss: 1.50732902       [[val]]     loss: 1.61028911  \n",
      "[[08/31/2017 04:48:11 PM]] [[step       42]]     [[train]]     loss: 1.51878649       [[val]]     loss: 1.60566488  \n",
      "[[08/31/2017 04:48:12 PM]] [[step       43]]     [[train]]     loss: 1.52972316       [[val]]     loss: 1.61462704  \n",
      "[[08/31/2017 04:48:13 PM]] [[step       44]]     [[train]]     loss: 1.54017375       [[val]]     loss: 1.58011355  \n",
      "[[08/31/2017 04:48:13 PM]] [[step       45]]     [[train]]     loss: 1.53092214       [[val]]     loss: 1.58924152  \n",
      "[[08/31/2017 04:48:14 PM]] [[step       46]]     [[train]]     loss: 1.54090252       [[val]]     loss: 1.59798106  \n",
      "[[08/31/2017 04:48:14 PM]] [[step       47]]     [[train]]     loss: 1.51231034       [[val]]     loss: 1.60635646  \n",
      "[[08/31/2017 04:48:16 PM]] [[step       48]]     [[train]]     loss: 1.52226319       [[val]]     loss: 1.57771757  \n",
      "[[08/31/2017 04:48:17 PM]] [[step       49]]     [[train]]     loss: 1.53181793       [[val]]     loss: 1.54656321  \n",
      "[[08/31/2017 04:48:18 PM]] [[step       50]]     [[train]]     loss: 1.54088943       [[val]]     loss: 1.55545413  \n",
      "[[08/31/2017 04:48:19 PM]] [[step       51]]     [[train]]     loss: 1.51164155       [[val]]     loss: 1.56400309  \n",
      "[[08/31/2017 04:48:19 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/31/2017 04:48:23 PM]] [[step       52]]     [[train]]     loss: 1.52085586       [[val]]     loss: 1.53897211  \n",
      "[[08/31/2017 04:48:23 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/31/2017 04:48:26 PM]] [[step       53]]     [[train]]     loss: 1.5297289        [[val]]     loss: 1.537038    \n",
      "[[08/31/2017 04:48:26 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/31/2017 04:48:28 PM]] [[step       54]]     [[train]]     loss: 1.5131375        [[val]]     loss: 1.51070986  \n",
      "[[08/31/2017 04:48:28 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/31/2017 04:48:31 PM]] [[step       55]]     [[train]]     loss: 1.52183148       [[val]]     loss: 1.50006011  \n",
      "[[08/31/2017 04:48:31 PM]] saving model to ./tf-data/checkpoints/model\n",
      "[[08/31/2017 04:48:34 PM]] [[step       56]]     [[train]]     loss: 1.5302204        [[val]]     loss: 1.50883098  \n",
      "[[08/31/2017 04:48:35 PM]] [[step       57]]     [[train]]     loss: 1.53832005       [[val]]     loss: 1.51729941  \n",
      "[[08/31/2017 04:48:36 PM]] [[step       58]]     [[train]]     loss: 1.51258581       [[val]]     loss: 1.52548078  \n",
      "[[08/31/2017 04:48:37 PM]] [[step       59]]     [[train]]     loss: 1.52070938       [[val]]     loss: 1.53338943  \n",
      "[[08/31/2017 04:48:37 PM]] [[step       60]]     [[train]]     loss: 1.5285666        [[val]]     loss: 1.54103879  \n",
      "[[08/31/2017 04:48:38 PM]] [[step       61]]     [[train]]     loss: 1.53617037       [[val]]     loss: 1.54844139  \n",
      "[[08/31/2017 04:48:39 PM]] [[step       62]]     [[train]]     loss: 1.54353274       [[val]]     loss: 1.55560898  \n",
      "[[08/31/2017 04:48:39 PM]] [[step       63]]     [[train]]     loss: 1.55066504       [[val]]     loss: 1.56255259  \n",
      "[[08/31/2017 04:48:40 PM]] [[step       64]]     [[train]]     loss: 1.55757789       [[val]]     loss: 1.56928255  \n",
      "[[08/31/2017 04:48:41 PM]] [[step       65]]     [[train]]     loss: 1.56428125       [[val]]     loss: 1.57580858  \n",
      "[[08/31/2017 04:48:42 PM]] [[step       66]]     [[train]]     loss: 1.56657285       [[val]]     loss: 1.57389896  \n",
      "[[08/31/2017 04:48:42 PM]] [[step       67]]     [[train]]     loss: 1.5467004        [[val]]     loss: 1.56521745  \n",
      "[[08/31/2017 04:48:43 PM]] [[step       68]]     [[train]]     loss: 1.52562994       [[val]]     loss: 1.5707548   \n",
      "[[08/31/2017 04:48:45 PM]] [[step       69]]     [[train]]     loss: 1.53240666       [[val]]     loss: 1.57688687  \n",
      "[[08/31/2017 04:48:45 PM]] [[step       70]]     [[train]]     loss: 1.51110515       [[val]]     loss: 1.58284621  \n",
      "[[08/31/2017 04:48:46 PM]] [[step       71]]     [[train]]     loss: 1.51680988       [[val]]     loss: 1.56716264  \n",
      "[[08/31/2017 04:48:48 PM]] [[step       72]]     [[train]]     loss: 1.503852         [[val]]     loss: 1.57309192  \n",
      "[[08/31/2017 04:48:50 PM]] [[step       73]]     [[train]]     loss: 1.48539159       [[val]]     loss: 1.57886095  \n",
      "[[08/31/2017 04:48:51 PM]] [[step       74]]     [[train]]     loss: 1.49225304       [[val]]     loss: 1.58447614  \n",
      "[[08/31/2017 04:48:52 PM]] [[step       75]]     [[train]]     loss: 1.49893391       [[val]]     loss: 1.56389092  \n",
      "[[08/31/2017 04:48:53 PM]] [[step       76]]     [[train]]     loss: 1.50058453       [[val]]     loss: 1.56955468  \n",
      "[[08/31/2017 04:48:54 PM]] [[step       77]]     [[train]]     loss: 1.48209136       [[val]]     loss: 1.57507321  \n",
      "[[08/31/2017 04:48:56 PM]] [[step       78]]     [[train]]     loss: 1.4666655        [[val]]     loss: 1.58045203  \n",
      "[[08/31/2017 04:48:57 PM]] [[step       79]]     [[train]]     loss: 1.46417522       [[val]]     loss: 1.58569638  \n",
      "[[08/31/2017 04:48:58 PM]] [[step       80]]     [[train]]     loss: 1.46372853       [[val]]     loss: 1.59081124  \n",
      "[[08/31/2017 04:48:59 PM]] [[step       81]]     [[train]]     loss: 1.47026842       [[val]]     loss: 1.59580134  \n",
      "[[08/31/2017 04:49:01 PM]] [[step       82]]     [[train]]     loss: 1.47665073       [[val]]     loss: 1.59988487  \n",
      "[[08/31/2017 04:49:01 PM]] [[step       83]]     [[train]]     loss: 1.46792806       [[val]]     loss: 1.58107672  \n",
      "[[08/31/2017 04:49:02 PM]] [[step       84]]     [[train]]     loss: 1.45089361       [[val]]     loss: 1.57969995  \n",
      "[[08/31/2017 04:49:03 PM]] [[step       85]]     [[train]]     loss: 1.44221855       [[val]]     loss: 1.58458716  \n",
      "[[08/31/2017 04:49:05 PM]] [[step       86]]     [[train]]     loss: 1.4333893        [[val]]     loss: 1.58936202  \n",
      "[[08/31/2017 04:49:06 PM]] [[step       87]]     [[train]]     loss: 1.42122254       [[val]]     loss: 1.59402836  \n",
      "[[08/31/2017 04:49:07 PM]] [[step       88]]     [[train]]     loss: 1.42772565       [[val]]     loss: 1.59089825  \n",
      "[[08/31/2017 04:49:08 PM]] [[step       89]]     [[train]]     loss: 1.42092592       [[val]]     loss: 1.59544383  \n",
      "[[08/31/2017 04:49:09 PM]] [[step       90]]     [[train]]     loss: 1.42728937       [[val]]     loss: 1.5998895   \n",
      "[[08/31/2017 04:49:10 PM]] [[step       91]]     [[train]]     loss: 1.43351449       [[val]]     loss: 1.60011945  \n",
      "[[08/31/2017 04:49:12 PM]] [[step       92]]     [[train]]     loss: 1.43115784       [[val]]     loss: 1.60441924  \n",
      "[[08/31/2017 04:49:12 PM]] [[step       93]]     [[train]]     loss: 1.43720936       [[val]]     loss: 1.60862755  \n",
      "[[08/31/2017 04:49:13 PM]] [[step       94]]     [[train]]     loss: 1.44313347       [[val]]     loss: 1.61274726  \n",
      "[[08/31/2017 04:49:15 PM]] [[step       95]]     [[train]]     loss: 1.44893416       [[val]]     loss: 1.59800404  \n",
      "[[08/31/2017 04:49:15 PM]] [[step       96]]     [[train]]     loss: 1.4401739        [[val]]     loss: 1.60214833  \n",
      "[[08/31/2017 04:49:16 PM]] [[step       97]]     [[train]]     loss: 1.4306549        [[val]]     loss: 1.59365684  \n",
      "[[08/31/2017 04:49:17 PM]] [[step       98]]     [[train]]     loss: 1.43640586       [[val]]     loss: 1.59776132  \n",
      "[[08/31/2017 04:49:18 PM]] [[step       99]]     [[train]]     loss: 1.42974873       [[val]]     loss: 1.6017837   \n",
      "[[08/31/2017 04:49:18 PM]] Training ended\n",
      "[[08/31/2017 04:49:18 PM]] Best validation loss of 1.50006011 at training step 55\n"
     ]
    }
   ],
   "source": [
    "# 100 steps make an epoch\n",
    "nn = WikiRNN(\n",
    "    name='v1',\n",
    "    reader=reader,\n",
    "    work_dir='./tf-data',\n",
    "    optimizer='adam',\n",
    "    learning_rate=.0001,\n",
    "    batch_size=1,\n",
    "    num_validation_batches=1,\n",
    "    num_training_steps=100,\n",
    "    early_stopping_steps=300,\n",
    "    num_restarts=3,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    enable_parameter_averaging=False,\n",
    "    min_steps_to_checkpoint=50,\n",
    "    loss_averaging_window=100,\n",
    "    log_interval=1,\n",
    "\n",
    "    state_size=10,\n",
    "    keep_prob=1\n",
    ")\n",
    "for step in nn.fit(yield_interval=10):\n",
    "    continue\n",
    "    preds = nn.predict(num_batches=1)\n",
    "    for i in range(1):\n",
    "        idx = np.random.randint(0, 128)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.ylim((0, 2))\n",
    "        \n",
    "        true = data[idx, :]\n",
    "        plt.plot(true, 'g--')\n",
    "        pred = preds['preds'][idx, :]\n",
    "        plt.plot(pred, 'k.')\n",
    "        \n",
    "        loss = np.abs(true - pred) * 2 / (true + pred + 1e-10)\n",
    "        plt.plot(loss, 'r:')\n",
    "        plt.plot([550-62], [0], 'ro')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nn.restore()\n",
    "# preds = nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

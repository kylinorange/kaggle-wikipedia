{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time\n",
    "from subprocess import check_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_df.csv\n",
      "key_1.csv\n",
      "median_49.csv\n",
      "median_weekend.csv\n",
      "page_df.csv\n",
      "page_ohe.csv\n",
      "sample_submission_1.csv\n",
      "spider.txt\n",
      "train_1.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_paths = [\n",
    "    \"/data/kaggle-wikipedia/data/\",\n",
    "    \"/Users/jiayou/Dropbox/JuanCode/Kaggle/Wikipedia/data/\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/JuanCode/Kaggle/Wikipedia/data/\"\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "print(check_output([\"ls\", root]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(root + 'train_1.csv')\n",
    "train.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = train.iloc[:,1:].values\n",
    "\n",
    "# 1. percentile scaler\n",
    "# scale = np.percentile(data, 98, axis=1)\n",
    "# data = data / np.maximum(1, scale.reshape((data.shape[0], 1))) - 0.5\n",
    "\n",
    "# 2. global std scaler\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(data.reshape((-1, 1)))\n",
    "# data = sc.transform(data.reshape((-1, 1))).reshape(data.shape)\n",
    "\n",
    "# 3. per page log1p + std scaler\n",
    "data = np.log1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpage = pd.read_csv(os.path.join(root, 'page_df.csv'))\n",
    "fdate = pd.read_csv(os.path.join(root, 'date_df.csv'))\n",
    "\n",
    "fdate.isweekday = fdate.isweekday.astype(np.int32)\n",
    "fdate.drop(['date', 'date_str'], axis=1, inplace=True)\n",
    "\n",
    "fpage_depth = {}\n",
    "for c in ['domain', 'access', 'agent']:\n",
    "    values = fpage[c].unique()\n",
    "    n = len(values)\n",
    "    fpage_depth[c] = n\n",
    "    idx_lookup = {values[i]: i for i in range(n)}\n",
    "    def v2idx(v):\n",
    "        return idx_lookup[v]\n",
    "    fpage[c] = np.vectorize(v2idx)(fpage[c])\n",
    "fpage.drop(['Page'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os, imp\n",
    "import pprint as pp\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def shape(tensor, dim=None):\n",
    "    \"\"\"Get tensor shape/dimension as list/int\"\"\"\n",
    "    if dim is None:\n",
    "        return tensor.shape.as_list()\n",
    "    else:\n",
    "        return tensor.shape.as_list()[dim]\n",
    "\n",
    "class TFBaseModel(object):\n",
    "\n",
    "    \"\"\"Interface containing some boilerplate code for training tensorflow models.\n",
    "    Subclassing models must implement self.calculate_loss(), which returns a tensor for the batch loss.\n",
    "    Code for the training loop, parameter updates, checkpointing, and inference are implemented here and\n",
    "    subclasses are mainly responsible for building the computational graph beginning with the placeholders\n",
    "    and ending with the loss tensor.\n",
    "    Args:\n",
    "        reader: Class with attributes train_batch_generator, val_batch_generator, and test_batch_generator\n",
    "            that yield dictionaries mapping tf.placeholder names (as strings) to batch data (numpy arrays).\n",
    "        batch_size: Minibatch size.\n",
    "        learning_rate: Learning rate.\n",
    "        optimizer: 'rms' for RMSProp, 'adam' for Adam, 'sgd' for SGD\n",
    "        grad_clip: Clip gradients elementwise to have norm at most equal to grad_clip.\n",
    "        regularization_constant:  Regularization constant applied to all trainable parameters.\n",
    "        early_stopping_steps:  Number of steps to continue training after validation loss has\n",
    "            stopped decreasing.\n",
    "        warm_start_init_step:  If nonzero, model will resume training a restored model beginning\n",
    "            at warm_start_init_step.\n",
    "        num_restarts:  After validation loss plateaus, the best checkpoint will be restored and the\n",
    "            learning rate will be halved.  This process will repeat num_restarts times.\n",
    "        enable_parameter_averaging:  If true, model saves exponential weighted averages of parameters\n",
    "            to separate checkpoint file.\n",
    "        min_steps_to_checkpoint:  Model only saves after min_steps_to_checkpoint training steps\n",
    "            have passed.\n",
    "        log_interval:  Train and validation accuracies are logged every log_interval training steps.\n",
    "        loss_averaging_window:  Train/validation losses are averaged over the last loss_averaging_window\n",
    "            training steps.\n",
    "        num_validation_batches:  Number of batches to be used in validation evaluation at each step.\n",
    "        log_dir: Directory where logs are written.\n",
    "        checkpoint_dir: Directory where checkpoints are saved.\n",
    "        prediction_dir: Directory where predictions/outputs are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reader,\n",
    "        batch_size=128,\n",
    "        num_training_steps=20000,\n",
    "        learning_rate=.01,\n",
    "        optimizer='adam',\n",
    "        grad_clip=5,\n",
    "        regularization_constant=0.0,\n",
    "        early_stopping_steps=3000,\n",
    "        warm_start_init_step=0,\n",
    "        num_restarts=None,\n",
    "        enable_parameter_averaging=False,\n",
    "        min_steps_to_checkpoint=100,\n",
    "        log_interval=20,\n",
    "        loss_averaging_window=100,\n",
    "        num_validation_batches=1,\n",
    "        work_dir='tf-data',\n",
    "        name='nn'\n",
    "    ):\n",
    "\n",
    "        self.reader = reader\n",
    "        self.batch_size = batch_size\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_clip = grad_clip\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.warm_start_init_step = warm_start_init_step\n",
    "        self.early_stopping_steps = early_stopping_steps if early_stopping_steps is not None else np.inf\n",
    "        self.enable_parameter_averaging = enable_parameter_averaging\n",
    "        self.num_restarts = num_restarts\n",
    "        self.min_steps_to_checkpoint = min_steps_to_checkpoint\n",
    "        self.log_interval = log_interval\n",
    "        self.num_validation_batches = num_validation_batches\n",
    "        self.loss_averaging_window = loss_averaging_window\n",
    "        self.name = name\n",
    "\n",
    "        self.log_dir = os.path.join(work_dir, 'logs')\n",
    "        self.prediction_dir = os.path.join(work_dir, 'predictions')\n",
    "        self.checkpoint_dir = os.path.join(work_dir, 'checkpoints')\n",
    "        if self.enable_parameter_averaging:\n",
    "            self.checkpoint_dir_averaged = os.path.join(work_dir, 'checkpoints-avg')\n",
    "\n",
    "        self.init_logging(self.log_dir)\n",
    "        self.logger.info('\\nNetwork hyper-parameters:\\n{}'.format(pp.pformat(self.__dict__)))\n",
    "        self.reader.describe(self.logger)\n",
    "\n",
    "        self.graph = self.build_graph()\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        print('Built graph')\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        raise NotImplementedError('subclass must implement this')\n",
    "\n",
    "    def fit(self, yield_interval=None):\n",
    "        with self.session.as_default():\n",
    "\n",
    "            if self.warm_start_init_step:\n",
    "                self.restore(self.warm_start_init_step)\n",
    "                step = self.warm_start_init_step\n",
    "            else:\n",
    "                self.session.run(self.init)\n",
    "                step = 0\n",
    "\n",
    "            train_generator = self.reader.train_batch_generator(self.batch_size)\n",
    "            val_generator = self.reader.val_batch_generator(self.num_validation_batches*self.batch_size)\n",
    "\n",
    "            train_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "            val_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "\n",
    "            best_validation_loss, best_validation_tstep = float('inf'), None\n",
    "            restarts = 0\n",
    "\n",
    "            while step < self.num_training_steps:\n",
    "                \n",
    "                if yield_interval is not None and step % yield_interval == 0:\n",
    "                    plt.figure(figsize=(20, 5))\n",
    "                    plt.plot(train_loss_history, ':')\n",
    "                    plt.plot(val_loss_history, 'g')\n",
    "                    plt.show()\n",
    "                    yield step\n",
    "\n",
    "                # validation evaluation\n",
    "                val_batch_df = next(val_generator)\n",
    "                val_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in val_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                val_feed_dict.update({self.learning_rate_var: self.learning_rate})\n",
    "                [val_loss] = self.session.run(\n",
    "                    fetches=[self.loss],\n",
    "                    feed_dict=val_feed_dict\n",
    "                )\n",
    "                val_loss_history.append(val_loss)\n",
    "\n",
    "                # train step\n",
    "                train_batch_df = next(train_generator)\n",
    "                train_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in train_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                train_feed_dict.update({self.learning_rate_var: self.learning_rate})\n",
    "                train_loss, _ = self.session.run(\n",
    "                    fetches=[self.loss, self.step],\n",
    "                    feed_dict=train_feed_dict\n",
    "                )\n",
    "                train_loss_history.append(train_loss)\n",
    "\n",
    "                if step % self.log_interval == 0 or step + 1 == self.num_training_steps:\n",
    "                    avg_train_loss = sum(train_loss_history) / len(train_loss_history)\n",
    "                    avg_val_loss = sum(val_loss_history) / len(val_loss_history)\n",
    "                    metric_log = (\n",
    "                        \"[[step {:>8}]]     \"\n",
    "                        \"[[train]]     loss: {:<12}     \"\n",
    "                        \"[[val]]     loss: {:<12}\"\n",
    "                    ).format(step, round(avg_train_loss, 8), round(avg_val_loss, 8))\n",
    "                    self.logger.info(metric_log)\n",
    "\n",
    "                    if step > self.min_steps_to_checkpoint and avg_val_loss < best_validation_loss:\n",
    "                        best_validation_loss = avg_val_loss\n",
    "                        best_validation_tstep = step\n",
    "                        self.save(step)\n",
    "                        if self.enable_parameter_averaging:\n",
    "                            self.save(step, averaged=True)\n",
    "\n",
    "                    if best_validation_tstep is not None and step - best_validation_tstep >= self.early_stopping_steps:\n",
    "\n",
    "                        if self.num_restarts is None or restarts >= self.num_restarts:\n",
    "                            self.logger.info('Early stopping')\n",
    "                            break\n",
    "\n",
    "                        if restarts < self.num_restarts:\n",
    "                            self.logger.info('')\n",
    "                            self.restore(best_validation_tstep)\n",
    "                            self.learning_rate /= 2.0\n",
    "#                             self.early_stopping_steps /= 2\n",
    "                            step = best_validation_tstep\n",
    "                            train_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "                            val_loss_history = deque(maxlen=self.loss_averaging_window)\n",
    "                            restarts += 1\n",
    "                            self.logger.info(\n",
    "                                'Half learning rate to {} and restore step {}'.format(self.learning_rate, step)\n",
    "                            )\n",
    "\n",
    "                step += 1\n",
    "\n",
    "            if step <= self.min_steps_to_checkpoint:\n",
    "                best_validation_tstep = step\n",
    "                self.save(step)\n",
    "                if self.enable_parameter_averaging:\n",
    "                    self.save(step, averaged=True)\n",
    "\n",
    "            self.logger.info('Training ended')\n",
    "            self.logger.info(\n",
    "                'Best validation loss of {} at training step {}'.format(\n",
    "                    round(best_validation_loss, 8), \n",
    "                    best_validation_tstep\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def predict(self, batch_size=128, num_batches=None):\n",
    "        if not os.path.isdir(self.prediction_dir):\n",
    "            os.makedirs(self.prediction_dir)\n",
    "\n",
    "        preds = {}\n",
    "        if hasattr(self, 'prediction_tensors'):\n",
    "            prediction_dict = {tensor_name: [] for tensor_name in self.prediction_tensors}\n",
    "\n",
    "            test_generator = self.reader.test_batch_generator(batch_size)\n",
    "            for i, test_batch_df in enumerate(test_generator):\n",
    "                test_feed_dict = {\n",
    "                    getattr(self, placeholder_name, None): data\n",
    "                    for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n",
    "                }\n",
    "\n",
    "                tensor_names, tf_tensors = zip(*self.prediction_tensors.items())\n",
    "                np_tensors = self.session.run(\n",
    "                    fetches=tf_tensors,\n",
    "                    feed_dict=test_feed_dict\n",
    "                )\n",
    "                for tensor_name, tensor in zip(tensor_names, np_tensors):\n",
    "                    prediction_dict[tensor_name].append(tensor)\n",
    "                    \n",
    "                if num_batches is not None and i + 1 == num_batches:\n",
    "                    break\n",
    "\n",
    "            for tensor_name, tensor in prediction_dict.items():\n",
    "                np_tensor = np.concatenate(tensor, 0)\n",
    "                preds[tensor_name] = np_tensor\n",
    "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "                self.logger.info('Saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "                np.save(save_file, np_tensor)\n",
    "\n",
    "        if hasattr(self, 'parameter_tensors'):\n",
    "            for tensor_name, tensor in self.parameter_tensors.items():\n",
    "                np_tensor = tensor.eval(self.session)\n",
    "\n",
    "                save_file = os.path.join(self.prediction_dir, '{}.npy'.format(tensor_name))\n",
    "                self.logger.info('Saving {} with shape {} to {}'.format(tensor_name, np_tensor.shape, save_file))\n",
    "                np.save(save_file, np_tensor)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def save(self, step, averaged=False):\n",
    "        saver = self.saver_averaged if averaged else self.saver\n",
    "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
    "        if not os.path.isdir(checkpoint_dir):\n",
    "            self.logger.info('creating checkpoint directory {}'.format(checkpoint_dir))\n",
    "            os.mkdir(checkpoint_dir)\n",
    "\n",
    "        model_path = os.path.join(checkpoint_dir, 'model')\n",
    "        self.logger.info('saving model to {}'.format(model_path))\n",
    "        saver.save(self.session, model_path, global_step=step)\n",
    "\n",
    "    def restore(self, step=None, averaged=False):\n",
    "        saver = self.saver_averaged if averaged else self.saver\n",
    "        checkpoint_dir = self.checkpoint_dir_averaged if averaged else self.checkpoint_dir\n",
    "        if not step:\n",
    "            model_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "            self.logger.info('Restoring model parameters from {}'.format(model_path))\n",
    "            saver.restore(self.session, model_path)\n",
    "        else:\n",
    "            model_path = os.path.join(\n",
    "                checkpoint_dir, 'model{}-{}'.format('_avg' if averaged else '', step)\n",
    "            )\n",
    "            self.logger.info('Restoring model from {}'.format(model_path))\n",
    "            saver.restore(self.session, model_path)\n",
    "\n",
    "    def init_logging(self, log_dir):\n",
    "        if not os.path.isdir(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        logger = logging.getLogger(\n",
    "            '{}.{}'.format(\n",
    "                type(self).__name__, \n",
    "                datetime.now().strftime('%Y-%m-%d.%H-%M-%S.%f')\n",
    "            )\n",
    "        )\n",
    "        logger.setLevel(logging.INFO)\n",
    "        fmtr = logging.Formatter(\n",
    "            fmt='[[%(asctime)s]] %(message)s',\n",
    "            datefmt='%m/%d/%Y %I:%M:%S %p'\n",
    "        )\n",
    "\n",
    "        h = logging.StreamHandler(stream=sys.stdout)\n",
    "        h.setFormatter(fmtr)\n",
    "        logger.addHandler(h)\n",
    "        \n",
    "        date_str = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        log_file = 'log.{}.{}.{}.txt'.format(type(self).__name__, self.name, date_str)\n",
    "        h = logging.FileHandler(filename=os.path.join(log_dir, log_file))\n",
    "        h.setFormatter(fmtr)\n",
    "        logger.addHandler(h)\n",
    "        \n",
    "        self.logger = logger\n",
    "\n",
    "    def update_parameters(self, loss):\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.learning_rate_var = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        if self.regularization_constant != 0:\n",
    "            l2_norm = tf.reduce_sum([tf.sqrt(tf.reduce_sum(tf.square(param))) for param in tf.trainable_variables()])\n",
    "            loss = loss + self.regularization_constant*l2_norm\n",
    "\n",
    "        optimizer = self.get_optimizer(self.learning_rate_var)\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        clipped = [\n",
    "            (g if g is None else tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v_) for g, v_ in grads\n",
    "        ]\n",
    "\n",
    "        step = optimizer.apply_gradients(clipped, global_step=self.global_step)\n",
    "\n",
    "        if self.enable_parameter_averaging:\n",
    "            maintain_averages_op = self.ema.apply(tf.trainable_variables())\n",
    "            with tf.control_dependencies([step]):\n",
    "                self.step = tf.group(maintain_averages_op)\n",
    "        else:\n",
    "            self.step = step\n",
    "\n",
    "    def log_parameters(self):\n",
    "        self.logger.info(\n",
    "            (\n",
    "                '\\n\\n'\n",
    "                'All parameters:\\n{}\\n'\n",
    "                'Trainable parameters:\\n{}\\n'\n",
    "                'Trainable parameter count: {}\\n'\n",
    "            ).format(\n",
    "                pp.pformat([(var.name, shape(var)) for var in tf.global_variables()]),\n",
    "                pp.pformat([(var.name, shape(var)) for var in tf.trainable_variables()]),\n",
    "                str(np.sum(np.prod(shape(var)) for var in tf.trainable_variables())),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_optimizer(self, learning_rate):\n",
    "        if self.optimizer == 'adam':\n",
    "            return tf.train.AdamOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'gd':\n",
    "            return tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif self.optimizer == 'rms':\n",
    "            return tf.train.RMSPropOptimizer(learning_rate, decay=0.95, momentum=0.9)\n",
    "        else:\n",
    "            assert False, 'optimizer must be adam, gd, or rms'\n",
    "\n",
    "    def build_graph(self):\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            self.ema = tf.train.ExponentialMovingAverage(decay=0.995)\n",
    "\n",
    "            self.loss = self.calculate_loss()\n",
    "            self.update_parameters(self.loss)\n",
    "            self.log_parameters()\n",
    "\n",
    "            self.saver = tf.train.Saver(max_to_keep=1)\n",
    "            if self.enable_parameter_averaging:\n",
    "                self.saver_averaged = tf.train.Saver(self.ema.variables_to_restore(), max_to_keep=1)\n",
    "\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "            return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self, data, fpage, fdate, seed=923):\n",
    "        self.data = data\n",
    "        self.seed = seed\n",
    "        self.days = self.data.shape[1]\n",
    "        \n",
    "        self.train_days = 400\n",
    "        self.max_train_days = 100\n",
    "        \n",
    "        self.domain = fpage.domain.values\n",
    "        self.access = fpage.access.values\n",
    "        self.agent = fpage.agent.values\n",
    "        \n",
    "        self.dayofweek = fdate.dayofweek.values\n",
    "        self.isweekday = fdate.isweekday.values\n",
    "        self.month = fdate.month.values\n",
    "        \n",
    "    def describe(self, logger):\n",
    "        logger.info('')\n",
    "        logger.info('Data dimensions:')\n",
    "        logger.info('    [[data]] {}'.format(self.data.shape))\n",
    "        logger.info('Split seed = {}'.format(self.seed))\n",
    "        logger.info('')\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            self.data,\n",
    "            batch_size=batch_size,\n",
    "            start_day = 0,\n",
    "            total_days = self.train_days,\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            self.data,\n",
    "            batch_size=batch_size,\n",
    "            start_day = self.train_days,\n",
    "            total_days = self.days - self.train_days,\n",
    "        )\n",
    "    \n",
    "    def scale(self, data):\n",
    "        sc = StandardScaler()\n",
    "        return (sc.fit_transform(data.T).T, sc)\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        start = 0\n",
    "        n = self.data.shape[0]\n",
    "        while start < n:\n",
    "            batch = {}\n",
    "            idx = [i for i in range(start, min(start + batch_size, n))]\n",
    "            batch['data'], batch['scaler'] = self.scale(self.data[idx, :])\n",
    "            batch['given_days'] = self.days - 62\n",
    "            batch['no_loss_days'] = 0\n",
    "            batch['days'] = self.days\n",
    "            \n",
    "            batch['dayofweek'] = self.dayofweek\n",
    "            batch['isweekday'] = self.isweekday\n",
    "            batch['month'] = self.month\n",
    "            \n",
    "            batch['domain'] = self.domain[idx]\n",
    "            batch['agent'] = self.agent[idx]\n",
    "            batch['access'] = self.access[idx]\n",
    "            \n",
    "            yield batch\n",
    "            \n",
    "            start += batch_size\n",
    "\n",
    "    def batch_generator(self, data, batch_size, start_day, total_days):\n",
    "        while True:\n",
    "            idx = np.random.randint(0, data.shape[0], [batch_size])\n",
    "            start = np.random.randint(start_day, start_day + total_days - 60)\n",
    "            days = np.random.randint(60, min(self.max_train_days, start_day + total_days - start))\n",
    "            days_idx = [i for i in range(start, start + days)]\n",
    "            given_days = days - 1\n",
    "            no_loss_days = days - 1\n",
    "            \n",
    "            batch = {}\n",
    "            batch['data'], batch['scaler'] = self.scale(data[idx, :][:, days_idx])\n",
    "            \n",
    "            batch['given_days'] = given_days\n",
    "            batch['no_loss_days'] = no_loss_days\n",
    "            batch['days'] = days\n",
    "            \n",
    "            batch['dayofweek'] = self.dayofweek[days_idx]\n",
    "            batch['isweekday'] = self.isweekday[days_idx]\n",
    "            batch['month'] = self.month[days_idx]\n",
    "            \n",
    "            batch['domain'] = self.domain[idx]\n",
    "            batch['agent'] = self.agent[idx]\n",
    "            batch['access'] = self.access[idx]\n",
    "            \n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiRNN(TFBaseModel):\n",
    "\n",
    "    def __init__(self, state_size, keep_prob=1, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        super(type(self), self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        self.data = tf.placeholder(tf.float32, name='data')\n",
    "        self.given_days = tf.placeholder(tf.int32, name='given_days')\n",
    "        self.no_loss_days = tf.placeholder(tf.int32, name='no_loss_days')\n",
    "        self.days = tf.placeholder(tf.int32, name='days')\n",
    "        batch_size = tf.shape(self.data)[0]\n",
    "        \n",
    "#         batch_size = tf.Print(batch_size, [self.data], \"data\", summarize=1000)\n",
    "        \n",
    "        # Features\n",
    "        self.dayofweek = tf.placeholder(tf.int32, [None])\n",
    "        self.isweekday = tf.placeholder(tf.int32, [None])\n",
    "        self.month = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self.domain = tf.placeholder(tf.int32, [None])\n",
    "        self.agent = tf.placeholder(tf.int32, [None])\n",
    "        self.access = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        dayofweek_oh = tf.one_hot(self.dayofweek, 7)\n",
    "        isweekday_oh = tf.one_hot(self.isweekday, 2)\n",
    "        month = tf.one_hot(self.month, 13)\n",
    "        \n",
    "        domain = tf.one_hot(self.domain, 9)\n",
    "        agent = tf.one_hot(self.agent, 2)\n",
    "        access = tf.one_hot(self.access, 3)\n",
    "        \n",
    "        date_features = tf.concat(\n",
    "            [\n",
    "                dayofweek_oh,\n",
    "                isweekday_oh,\n",
    "                month,\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        date_features = tf.tile(tf.expand_dims(date_features, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        page_features = tf.concat(\n",
    "            [\n",
    "                domain,\n",
    "                agent,\n",
    "                access,\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        page_features = tf.tile(tf.expand_dims(page_features, 1), [1, self.days, 1])\n",
    "        \n",
    "        features = tf.concat([date_features, page_features], axis=2)\n",
    "        \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(\n",
    "                self.state_size\n",
    "            ),\n",
    "            output_keep_prob=self.keep_prob\n",
    "        )\n",
    "        \n",
    "        # [batch_size, state_size]\n",
    "        state = cell.zero_state(tf.shape(self.data)[0], dtype=tf.float32)\n",
    "        # [batch_size, 1]\n",
    "        last_output = tf.zeros([tf.shape(self.data)[0], 1], dtype=tf.float32)\n",
    "        \n",
    "        loss = tf.constant(0, dtype=tf.float32)\n",
    "        step = tf.constant(0, dtype=tf.int32)\n",
    "        output_ta = tf.TensorArray(size=self.days, dtype=tf.float32)\n",
    "        \n",
    "        def cond(last_output, state, loss, step, output_ta):\n",
    "            return step < self.days\n",
    "        \n",
    "        def body(last_output, state, loss, step, output_ta):\n",
    "            inp = tf.concat(\n",
    "                [\n",
    "                    last_output,\n",
    "                    features[:, step, :],\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "#             inp = tf.cond(\n",
    "#                 step < 10,\n",
    "#                 lambda: tf.Print(inp, [step, inp], \"input\", summarize=200*47),\n",
    "#                 lambda: inp,\n",
    "#             )\n",
    "            \n",
    "            output, state = cell(inp, state)\n",
    "            output = tf.layers.dense(\n",
    "                output,\n",
    "                1,\n",
    "                name='dense-top'\n",
    "            )\n",
    "            output_ta = output_ta.write(step, tf.transpose(output))\n",
    "            \n",
    "            last_output = tf.cond(\n",
    "                step < self.given_days,\n",
    "                lambda: tf.expand_dims(self.data[:,step], 1),\n",
    "                lambda: output,\n",
    "            )\n",
    "            last_output.set_shape([None, 1])\n",
    "            \n",
    "#             true = tf.maximum(1e-8, self.data[:,step])            \n",
    "            true = self.data[:,step]\n",
    "            loss = tf.cond(\n",
    "                step >= self.no_loss_days,\n",
    "#                 lambda: loss + tf.reduce_mean(2 * tf.abs(true - output) / tf.maximum(1e-6, true + output)),\n",
    "                lambda: loss + tf.reduce_mean(tf.abs(true - output)),\n",
    "                lambda: loss\n",
    "            )\n",
    "            loss.set_shape([])\n",
    "            \n",
    "            return (last_output, state, loss, step + 1, output_ta)\n",
    "        \n",
    "        _, _, loss, _, output_ta = tf.while_loop(\n",
    "            cond=cond,\n",
    "            body=body,\n",
    "            loop_vars=(last_output, state, loss, step, output_ta)\n",
    "        )\n",
    "        \n",
    "        self.preds = tf.transpose(output_ta.concat())\n",
    "        self.prediction_tensors = {\n",
    "            'preds': self.preds\n",
    "        }\n",
    "        \n",
    "        loss = loss / tf.cast(self.days - self.no_loss_days, tf.float32)\n",
    "#         loss = tf.Print(loss, [loss, self.data[:, -1], self.preds[:, -1]], \"Loss = \")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = DataReader(data, fpage=fpage, fdate=fdate, seed=923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1000 steps make an epoch\n",
    "nn = WikiRNN(\n",
    "    name='nn4',\n",
    "    reader=reader,\n",
    "    work_dir='./tf-data',\n",
    "    optimizer='adam',\n",
    "    learning_rate=.001,\n",
    "    batch_size=128,\n",
    "    num_validation_batches=1,\n",
    "    num_training_steps=10000,\n",
    "    early_stopping_steps=20000,\n",
    "    num_restarts=3,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    enable_parameter_averaging=False,\n",
    "    min_steps_to_checkpoint=1000,\n",
    "    loss_averaging_window=1000,\n",
    "    log_interval=100,\n",
    "\n",
    "    state_size=300,\n",
    "    keep_prob=1\n",
    ")\n",
    "\n",
    "def smape(true, pred, sc):\n",
    "    t = sc.inverse_transform(true.T)\n",
    "    p = sc.inverse_transform(pred.T)\n",
    "    p = np.maximum(0, p)\n",
    "    return (np.abs(t - p) * 200 / (t + p + 1e-10)).reshape((-1))\n",
    "    \n",
    "\n",
    "val_gen = reader.val_batch_generator(1000)\n",
    "    \n",
    "for step in nn.fit(yield_interval=1000):\n",
    "#     preds = nn.predict(num_batches=1)\n",
    "#     for i in range(1):\n",
    "#         idx = np.random.randint(0, 128)\n",
    "#         plt.figure(figsize=(20, 5))\n",
    "#         plt.ylim((-0.51, 2.1))\n",
    "        \n",
    "#         true = reader.scale(data[idx, :].reshape((1, -1))).reshape((-1))\n",
    "#         plt.plot(true, 'g--')\n",
    "#         pred = preds['preds'][idx, :]\n",
    "#         plt.plot(pred, 'k.')\n",
    "        \n",
    "#         loss = smape(true, pred)\n",
    "#         plt.plot(loss, 'r:')\n",
    "#         plt.plot([550-62], [0], 'ro')\n",
    "        \n",
    "#         plt.title('loss = {}'.format(loss[-62:].mean()))\n",
    "#         plt.show()\n",
    "        \n",
    "#     true = data[:128, -62]\n",
    "#     pred = preds['preds'][:128, -62]\n",
    "#     loss = smape(true, pred)\n",
    "#     print('First 128 smape loss = {}'.format(loss.mean()))\n",
    "    \n",
    "    val_batch_df = next(val_gen)\n",
    "    feed_dict = {\n",
    "        getattr(nn, placeholder_name, None): data\n",
    "        for placeholder_name, data in val_batch_df.items() if hasattr(nn, placeholder_name)\n",
    "    }\n",
    "\n",
    "#     train_feed_dict.update({nn.learning_rate_var: nn.learning_rate})\n",
    "    loss_l1, preds = nn.session.run(\n",
    "        fetches=[nn.loss, nn.preds],\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "    loss_smape = smape(val_batch_df['data'][:, [-1]], preds[:, [-1]], sc=val_batch_df['scaler']).mean()\n",
    "    print('1 day prediction val: smape = {}, l1 = {}'.format(loss_smape, loss_l1))\n",
    "    \n",
    "    for i in range(1):\n",
    "        idx = np.random.randint(0, 1000)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        \n",
    "        true = val_batch_df['data'][idx, :]\n",
    "        plt.plot(true, 'g--')\n",
    "        pred = preds[idx, :]\n",
    "        plt.plot(pred, 'k.')\n",
    "        \n",
    "#         loss = smape(true, pred)\n",
    "#         plt.plot(loss, 'r:')\n",
    "#         plt.title('loss = {}'.format(loss[-62:].mean()))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nn.restore()\n",
    "# preds = nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
